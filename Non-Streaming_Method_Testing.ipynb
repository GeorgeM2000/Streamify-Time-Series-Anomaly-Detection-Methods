{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from CUSUM import CUSUM\n",
    "from tqdm.notebook import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TSB_UAD.models.distance import Fourier\n",
    "from TSB_UAD.models.feature import Window\n",
    "from TSB_UAD.utils.slidingWindows import find_length, plotFig, printResult\n",
    "\n",
    "from TSB_UAD.models.iforest import IForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Non-Streaming Methods***\n",
    "We pick non-streaming methods of our choice as a baseline. We experiment on the generated dataset of time-series. In the non-streaming setting we use the entire generated files as input. We later modify the methods to operate in a streaming setting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Data Pre-Processing***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Dataset A***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two code cells are used to load the dataset with time-series from the following domains:\n",
    "- Occupancy\n",
    "- SensorScope\n",
    "- NAB\n",
    "- NASA-MSL\n",
    "- SMD\n",
    "- YAHOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Time-Series dictionary\n",
    "with open('Time-Series_Data_Dictionaries/Time-Series-Random-Data-of-Interest-Dictionary.json', 'r') as json_file:\n",
    "    loaded_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some info about the generated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts1: ['Normality_1', 'SensorScope']\n",
      "ts3: ['Normality_1', 'NASA-MSL']\n",
      "ts4: ['Normality_1', 'YAHOO']\n",
      "ts5: ['Normality_1', 'SMD']\n",
      "ts8: ['Normality_1', 'SMD']\n",
      "ts2: ['Normality_2', 'SensorScope', 'NAB']\n",
      "ts9: ['Normality_2', 'Occupancy', 'NASA-MSL']\n",
      "ts6: ['Normality_3', 'SensorScope', 'YAHOO', 'NASA-MSL']\n",
      "ts7: ['Normality_3', 'YAHOO', 'NASA-MSL', 'SMD']\n"
     ]
    }
   ],
   "source": [
    "for filename, info in loaded_dict.items():\n",
    "    print(f'{filename}: {info}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Dataset B***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next cell to load data from other domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data for the evaluation.\n",
    "all_data = []\n",
    "\n",
    "with open('dataset.pkl', 'rb') as f:\n",
    "    data = pkl.load(f)\n",
    "\n",
    "all_data.extend(data['evaluation']['single_normality'])\n",
    "all_data.extend(data['evaluation']['double_normality'])\n",
    "all_data.extend(data['evaluation']['triple_normality'])\n",
    "all_data.extend(data['evaluation']['quadruple_normality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Pre-processing for non-streaming methods***\n",
    "Simple data pre-processing based on TSB-UAD. This pre-processing serves as the pre-processing baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess first dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for filename, info in loaded_dict.items():\n",
    "for timeseries in all_data:\n",
    "\n",
    "    #ts_filepath = f\"TS-Data-Files/{filename}\"\n",
    "    #ts = pd.read_csv(ts_filepath, header=None).dropna().to_numpy()\n",
    "\n",
    "    #name = ts_filepath.split('/')[-1]\n",
    "    #max_length = ts.shape[0]\n",
    "\n",
    "    #data = ts[:max_length, 0].astype(float)\n",
    "    #label = ts[:max_length, 1]\n",
    "\n",
    "    name = timeseries['Name']\n",
    "\n",
    "    data = timeseries['data']\n",
    "    max_length = data.shape[0]\n",
    "    label = timeseries['labels']\n",
    "\n",
    "    slidingWindow = find_length(data)\n",
    "    X_data = Window(window=slidingWindow).convert(data).to_numpy()\n",
    "\n",
    "    print(f'Time-Series name: {name}')\n",
    "    print(\"Estimated Subsequence length: \", slidingWindow)\n",
    "    print()\n",
    "    \n",
    "    preprocessed_dict[name] = {\n",
    "        'name': name,\n",
    "        'data': data,\n",
    "        'label': label,\n",
    "        'slidingWindow': slidingWindow,\n",
    "        'X_data': X_data,\n",
    "        'Time series length': len(data),\n",
    "        'Number of abnormal points': list(label).count(1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Pre-processing for naive streaming variants of non-streaming methods***\n",
    "In a streaming setting we are unaware of the data size. Thus, we have to define batches/artitions of some points. In this simple naive variant, we assume we know the data size in order to properly define the number of points each partition will have. In other words, we naively say that each batch/partition will be have a specifc number of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5 # Number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for filename, info in loaded_dict.items():\n",
    "for timeseries in all_data:\n",
    "\n",
    "    #ts_filepath = f\"TS-Data-Files/{filename}\"\n",
    "    #ts = pd.read_csv(ts_filepath, header=None).dropna().to_numpy()\n",
    "\n",
    "    #name = ts_filepath.split('/')[-1]\n",
    "    #max_length = ts.shape[0]\n",
    "\n",
    "    #data = ts[:max_length, 0].astype(float)\n",
    "    #label = ts[:max_length, 1]\n",
    "\n",
    "    name = timeseries['Name']\n",
    "\n",
    "    data = timeseries['data']\n",
    "    max_length = data.shape[0]\n",
    "    label = timeseries['labels']\n",
    "    global_sw = find_length(data) # The sliding window calculated for all data\n",
    "\n",
    "    data_partitions = []\n",
    "    sliding_windows = []\n",
    "    x_data_partitions = []\n",
    "    x_data_partitions_len = 0\n",
    "\n",
    "    # Divide data based on n and add the remaining points to the last partition\n",
    "    partition_size = int(math.floor(len(data) / n))\n",
    "    remaining_points = len(data) - (partition_size * n)\n",
    "\n",
    "    for par in range(n):\n",
    "        start_idx = par * partition_size\n",
    "        end_idx = start_idx + partition_size\n",
    "        if par == n - 1:  \n",
    "            end_idx += remaining_points\n",
    "        data_partitions.append(data[start_idx:end_idx])\n",
    "    \n",
    "    # For each partition created, calculate the sliding window \n",
    "    for partition in data_partitions:\n",
    "        slidingWindow = find_length(partition)\n",
    "        sliding_windows.append(slidingWindow)\n",
    "\n",
    "        X_data = Window(window=slidingWindow).convert(partition).to_numpy()\n",
    "        x_data_partitions.append(X_data)\n",
    "        x_data_partitions_len += len(X_data)\n",
    "\n",
    "    \n",
    "    preprocessed_dict[filename] = {\n",
    "        'name': name,\n",
    "        'data': data,\n",
    "        'label': label,\n",
    "        'global_sliding_window': global_sw,\n",
    "        'slidingWindow': sliding_windows,\n",
    "        'X_data': x_data_partitions,\n",
    "        'X_data Length': x_data_partitions_len,\n",
    "        'Time series length': len(data),\n",
    "        'Number of abnormal points': list(label).count(1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Pre-processing for variations of naive streaming variants of non-streaming methods***\n",
    "Here, we partition data naively as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for filename, info in loaded_dict.items():\n",
    "for timeseries in all_data:\n",
    "\n",
    "    #ts_filepath = f\"TS-Data-Files/{filename}\"\n",
    "    #ts = pd.read_csv(ts_filepath, header=None).dropna().to_numpy()\n",
    "\n",
    "    #name = ts_filepath.split('/')[-1]\n",
    "    #max_length = ts.shape[0]\n",
    "\n",
    "    #data = ts[:max_length, 0].astype(float)\n",
    "    #label = ts[:max_length, 1]\n",
    "\n",
    "    name = timeseries['Name']\n",
    "\n",
    "    data = timeseries['data']\n",
    "    max_length = data.shape[0]\n",
    "    label = timeseries['labels']\n",
    "    global_sw = find_length(data)\n",
    "\n",
    "    data_partitions = []\n",
    "\n",
    "    partition_size = int(math.floor(len(data) / n))\n",
    "    remaining_points = len(data) - (partition_size * n)\n",
    "\n",
    "    for par in range(n):\n",
    "        start_idx = par * partition_size\n",
    "        end_idx = start_idx + partition_size\n",
    "        if par == n - 1:  \n",
    "            end_idx += remaining_points\n",
    "        data_partitions.append(data[start_idx:end_idx])\n",
    "    \n",
    "    preprocessed_dict[filename] = {\n",
    "        'name': name,\n",
    "        'data': data,\n",
    "        'label': label,\n",
    "        'data partitions': data_partitions,\n",
    "        'global_sliding_window': global_sw,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Pre-processing for variations of naive streaming variants of non-streaming methods***(Dynamic partitioning)\n",
    "Naively partitioning the data is not a reliable solution. We want to partition the data as soon as an abrupt change occurs. For that, we can use:\n",
    "- 1. MinMax range partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Dynamic Partitioning***(Variant 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for filename, info in loaded_dict.items():\n",
    "for timeseries in all_data:\n",
    "    #ts_filepath = f\"TS-Data-Files/{filename}\"\n",
    "    \n",
    "    # === Pre-processing steps ===\n",
    "\n",
    "    # Prepare data for unsupervised methods\n",
    "    #ts = pd.read_csv(ts_filepath, header=None).dropna().to_numpy()\n",
    "\n",
    "    #name = ts_filepath.split('/')[-1]\n",
    "    #max_length = ts.shape[0]\n",
    "\n",
    "    #data = ts[:max_length, 0].astype(float)\n",
    "    #label = ts[:max_length, 1]\n",
    "\n",
    "    name = timeseries['Name']\n",
    "    data = timeseries['data']\n",
    "    max_length = data.shape[0]\n",
    "    label = timeseries['labels']\n",
    "    global_sw = find_length(data)\n",
    "\n",
    "    initial_partition_length = int(len(data) * 0.25)\n",
    "    initial_partition = data[:initial_partition_length]\n",
    "\n",
    "    max = np.max(initial_partition)\n",
    "    min = np.min(initial_partition)\n",
    "\n",
    "    data_partitions = [initial_partition]\n",
    "    current_partition = []\n",
    "    change_detected = False\n",
    "\n",
    "    p = 500\n",
    "    change_point_threshold = 0.5\n",
    "    exceed_threshold = 0.65\n",
    "    post_change_points = []\n",
    "\n",
    "    for point in data[initial_partition_length:]:\n",
    "        \n",
    "        # Check for significant change\n",
    "        if (point > max * (1 + change_point_threshold)) or (point < min * (1 - change_point_threshold)):\n",
    "            change_detected = True\n",
    "     \n",
    "        current_partition.append(point)\n",
    "\n",
    "\n",
    "        # After change, collect additional points\n",
    "        if change_detected:\n",
    "            post_change_points.append(point)\n",
    "            if len(post_change_points) == p:\n",
    "                exceeds_threshold_points = [(pt > max * (1 + change_point_threshold) or pt < min * (1 - change_point_threshold)) for pt in post_change_points]\n",
    "                if sum(exceeds_threshold_points) >= exceed_threshold * p:\n",
    "                    max = np.mean([max] + [pt for pt in post_change_points if pt > max])\n",
    "                    min = np.mean([min] + [pt for pt in post_change_points if pt < min])\n",
    "\n",
    "                post_change_points = []\n",
    "\n",
    "                # Add the current partition to data partitions\n",
    "                data_partitions.append(np.array(current_partition))\n",
    "                current_partition = []\n",
    "                change_detected = False\n",
    "                \n",
    "        \n",
    "    # Add any remaining points in current_partition to data_partitions\n",
    "    if current_partition:\n",
    "        data_partitions.append(np.array(current_partition))\n",
    "\n",
    "    \n",
    "    preprocessed_dict[name] = {\n",
    "        'name': name,\n",
    "        'data': data,\n",
    "        'label': label,\n",
    "        'data partitions': data_partitions,\n",
    "        'global_sliding_window': global_sw,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the number of partitions created for each time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 310 for file: ECG1\n",
      "Number of partitions: 29 for file: ECG1_20k\n",
      "Number of partitions: 2 for file: IOPS1\n",
      "Number of partitions: 5 for file: SMD1\n",
      "Number of partitions: 2 for file: Occupancy1\n",
      "Number of partitions: 26 for file: ECG1+IOPS1\n",
      "Number of partitions: 6 for file: SMD1+Occupancy1\n",
      "Number of partitions: 26 for file: ECG1+IOPS1+Occupancy1\n",
      "Number of partitions: 45 for file: SMD1+ECG1+Occupancy1\n",
      "Number of partitions: 13 for file: ECG1+IOPS1+SMD1+Occupancy1\n"
     ]
    }
   ],
   "source": [
    "for filename in preprocessed_dict.keys():\n",
    "    ts = preprocessed_dict[filename]\n",
    "\n",
    "    print(f\"Number of partitions: {len(ts['data partitions'])} for file: {ts['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the size of the partitions consistent with the initial size of the time-series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of partitions: 229900 for file: ECG1. Original data size: 229900\n",
      "Total size of partitions: 20000 for file: ECG1_20k. Original data size: 20000\n",
      "Total size of partitions: 8784 for file: IOPS1. Original data size: 8784\n",
      "Total size of partitions: 28479 for file: SMD1. Original data size: 28479\n",
      "Total size of partitions: 2665 for file: Occupancy1. Original data size: 2665\n",
      "Total size of partitions: 28784 for file: ECG1+IOPS1. Original data size: 28784\n",
      "Total size of partitions: 31144 for file: SMD1+Occupancy1. Original data size: 31144\n",
      "Total size of partitions: 31449 for file: ECG1+IOPS1+Occupancy1. Original data size: 31449\n",
      "Total size of partitions: 51144 for file: SMD1+ECG1+Occupancy1. Original data size: 51144\n",
      "Total size of partitions: 59928 for file: ECG1+IOPS1+SMD1+Occupancy1. Original data size: 59928\n"
     ]
    }
   ],
   "source": [
    "for filename in preprocessed_dict.keys():\n",
    "    ts = preprocessed_dict[filename]\n",
    "    par_size = 0\n",
    "    for partition in ts['data partitions']:\n",
    "        par_size += len(partition)\n",
    "    \n",
    "    print(f\"Total size of partitions: {par_size} for file: {ts['name']}. Original data size: {len(ts['data'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in preprocessed_dict.keys():\n",
    "    ts = preprocessed_dict[filename]\n",
    "\n",
    "    if len(ts['data partitions']) < 10:\n",
    "\n",
    "        fig, axes = plt.subplots(1, len(ts['data partitions']), figsize=(20, 5))\n",
    "\n",
    "        for i, array in enumerate(ts['data partitions']):\n",
    "            axes[i].plot(array)\n",
    "            axes[i].set_title(f\"Partition {i+1} of {ts['name']}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Plot TS length and number of abnormal points***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get filenames, time series lengths, and number of abnormal points\n",
    "filenames = list(preprocessed_dict.keys())\n",
    "time_series_lengths = [data['Time series length'] for data in preprocessed_dict.values()]\n",
    "number_of_abnormal_points = [data['Number of abnormal points'] for data in preprocessed_dict.values()]\n",
    "\n",
    "# Plot 'Time series length' and 'Number of abnormal points' for each filename\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(filenames, time_series_lengths, marker='o', linestyle='-', color='skyblue')\n",
    "plt.xlabel('Filename')\n",
    "plt.ylabel('Time series length')\n",
    "plt.title('Time Series Length for Each Filename')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(filenames, number_of_abnormal_points, marker='o', linestyle='-', color='lightgreen')\n",
    "plt.xlabel('Filename')\n",
    "plt.ylabel('Number of abnormal points')\n",
    "plt.title('Number of Abnormal Points for Each Filename')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Anomaly Detection***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Information:\n",
    "- TN: The point is normal and we predicted it is normal\n",
    "- TP: The point is abnormal and we predicted it is abnormal\n",
    "- FP: The point is normal and we predicted it is abnormal\n",
    "- FN: The point is abnormal and we predicted it is normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = ['AUC', \n",
    "                'Precision', \n",
    "                'Recall', \n",
    "                'F', \n",
    "                'Rrecall', \n",
    "                'ExistenceReward',\n",
    "                'OverlapReward',\n",
    "                'Rprecision',\n",
    "                'Rf',\n",
    "                'Precision@k',\n",
    "                'R_AUC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to colorize the cells of the dataframe results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_diff(val):\n",
    "    color = ''\n",
    "    if val > 0:\n",
    "        color = 'background-color: lightgreen'\n",
    "    elif val < 0:\n",
    "        color = 'background-color: lightcoral'\n",
    "    return color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Isolation Forest***(Original)\n",
    "Non-Streaming Variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = 'IForest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for filename in preprocessed_dict.keys():\n",
    "    ts = preprocessed_dict[filename]\n",
    "    x = ts['X_data']\n",
    "    clf = IForest(n_jobs=7, random_state=42)\n",
    "\n",
    "    t0 = time()\n",
    "    clf.fit(x)\n",
    "    \n",
    "    \n",
    "    score = clf.decision_scores_\n",
    "    score = MinMaxScaler(feature_range=(0,1)).fit_transform(score.reshape(-1,1)).ravel()\n",
    "    score = np.array([score[0]]*math.ceil((ts['slidingWindow']-1)/2) + list(score) + [score[-1]]*((ts['slidingWindow']-1)//2))\n",
    "\n",
    "    t1 = time()\n",
    "    \n",
    "    # Plot figure\n",
    "    #plotFig(ts['data'], ts['label'], score, ts['slidingWindow'], fileName=ts['name'] + ' ' + loaded_dict[ts['name']][0], modelName=modelName)\n",
    "\n",
    "    # Calculate the results\n",
    "    L = printResult(ts['data'], ts['label'], score, ts['slidingWindow'], ts['name'], modelName)\n",
    "    #L = [ '%.2f' % elem for elem in L]\n",
    "    #results.append([filename] + L)\n",
    "    results.append([filename] + L + [t1-t0, len(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use for Dataset A\n",
    "columns = ['Filename'] + eval_metrics\n",
    "iforest_res = pd.DataFrame(results, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Name'] + ['AUC', 'Precision', 'Recall', 'F-score', 'Range-recall', 'ExistenceReward', 'OverlapReward', 'Range-precision', 'Range-Fscore', 'Precision@k', 'RangeAUC', 'Time', 'Number of Windows']\n",
    "df = pd.DataFrame(results, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Precision@k</th>\n",
       "      <th>Time</th>\n",
       "      <th>Number of Windows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ECG1</td>\n",
       "      <td>0.963406</td>\n",
       "      <td>0.208339</td>\n",
       "      <td>28.327885</td>\n",
       "      <td>229801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ECG1_20k</td>\n",
       "      <td>0.973288</td>\n",
       "      <td>0.669630</td>\n",
       "      <td>2.175788</td>\n",
       "      <td>19901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IOPS1</td>\n",
       "      <td>0.534240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.343871</td>\n",
       "      <td>8497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SMD1</td>\n",
       "      <td>0.845381</td>\n",
       "      <td>0.306236</td>\n",
       "      <td>3.862060</td>\n",
       "      <td>28355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Occupancy1</td>\n",
       "      <td>0.871266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.331349</td>\n",
       "      <td>2541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ECG1+IOPS1</td>\n",
       "      <td>0.809130</td>\n",
       "      <td>0.533485</td>\n",
       "      <td>3.188019</td>\n",
       "      <td>28685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SMD1+Occupancy1</td>\n",
       "      <td>0.833035</td>\n",
       "      <td>0.223404</td>\n",
       "      <td>4.172662</td>\n",
       "      <td>31020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ECG1+IOPS1+Occupancy1</td>\n",
       "      <td>0.882892</td>\n",
       "      <td>0.462493</td>\n",
       "      <td>3.424442</td>\n",
       "      <td>31350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SMD1+ECG1+Occupancy1</td>\n",
       "      <td>0.688620</td>\n",
       "      <td>0.223912</td>\n",
       "      <td>6.872895</td>\n",
       "      <td>51020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ECG1+IOPS1+SMD1+Occupancy1</td>\n",
       "      <td>0.651722</td>\n",
       "      <td>0.213767</td>\n",
       "      <td>6.749730</td>\n",
       "      <td>59829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Name       AUC  Precision@k       Time  \\\n",
       "0                        ECG1  0.963406     0.208339  28.327885   \n",
       "1                    ECG1_20k  0.973288     0.669630   2.175788   \n",
       "2                       IOPS1  0.534240     0.000000   2.343871   \n",
       "3                        SMD1  0.845381     0.306236   3.862060   \n",
       "4                  Occupancy1  0.871266     0.000000   0.331349   \n",
       "5                  ECG1+IOPS1  0.809130     0.533485   3.188019   \n",
       "6             SMD1+Occupancy1  0.833035     0.223404   4.172662   \n",
       "7       ECG1+IOPS1+Occupancy1  0.882892     0.462493   3.424442   \n",
       "8        SMD1+ECG1+Occupancy1  0.688620     0.223912   6.872895   \n",
       "9  ECG1+IOPS1+SMD1+Occupancy1  0.651722     0.213767   6.749730   \n",
       "\n",
       "   Number of Windows  \n",
       "0             229801  \n",
       "1              19901  \n",
       "2               8497  \n",
       "3              28355  \n",
       "4               2541  \n",
       "5              28685  \n",
       "6              31020  \n",
       "7              31350  \n",
       "8              51020  \n",
       "9              59829  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Number of anomalies'] = df['Name'].apply(lambda x: np.sum(preprocessed_dict[x]['label']))\n",
    "df[['Name', 'AUC', 'Precision@k', 'Time', 'Number of Windows']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "                      Name &      AUC &  Precision@k &      Time &  Number of Windows \\\\\n",
      "\\midrule\n",
      "                      ECG1 & 0.963406 &     0.208339 & 28.327885 &             229801 \\\\\n",
      "                  ECG1\\_20k & 0.973288 &     0.669630 &  2.175788 &              19901 \\\\\n",
      "                     IOPS1 & 0.534240 &     0.000000 &  2.343871 &               8497 \\\\\n",
      "                      SMD1 & 0.845381 &     0.306236 &  3.862060 &              28355 \\\\\n",
      "                Occupancy1 & 0.871266 &     0.000000 &  0.331349 &               2541 \\\\\n",
      "                ECG1+IOPS1 & 0.809130 &     0.533485 &  3.188019 &              28685 \\\\\n",
      "           SMD1+Occupancy1 & 0.833035 &     0.223404 &  4.172662 &              31020 \\\\\n",
      "     ECG1+IOPS1+Occupancy1 & 0.882892 &     0.462493 &  3.424442 &              31350 \\\\\n",
      "      SMD1+ECG1+Occupancy1 & 0.688620 &     0.223912 &  6.872895 &              51020 \\\\\n",
      "ECG1+IOPS1+SMD1+Occupancy1 & 0.651722 &     0.213767 &  6.749730 &              59829 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df[['Name', 'AUC', 'Precision@k', 'Time', 'Number of Windows']].to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use for Dataset A\n",
    "iforest_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Results/Isolation-Forest/Second_Dataset/IForest_Non-Streaming.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Isolation Forest***(Variant 1)\n",
    "Naive Streaming Variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = 'IForest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for filename in preprocessed_dict.keys():\n",
    "    ts = preprocessed_dict[filename]\n",
    "    clf = IForest(n_jobs=7, random_state=42)\n",
    "    total_time = 0\n",
    "\n",
    "    scores = []\n",
    "    for par in range(n):\n",
    "\n",
    "        t0 = time()\n",
    "        clf.fit(ts['X_data'][par])\n",
    "\n",
    "        score = clf.decision_scores_\n",
    "    \n",
    "        score = MinMaxScaler(feature_range=(0,1)).fit_transform(score.reshape(-1,1)).ravel()\n",
    "        score = np.array([score[0]]*math.ceil((ts['slidingWindow'][par]-1)/2) + list(score) + [score[-1]]*((ts['slidingWindow'][par]-1)//2))\n",
    "        \n",
    "        scores.extend(score)\n",
    "        t1 = time()\n",
    "\n",
    "        total_time += t1 - t0\n",
    "    \n",
    "    \n",
    "    scores = np.array(scores)\n",
    "    # Plot figure\n",
    "    #plotFig(ts['data'], ts['label'], scores, ts['global_sliding_window'], fileName=ts['name'] + ' ' + loaded_dict[ts['name']][0], modelName=modelName)\n",
    "\n",
    "    # Calculate the results\n",
    "    L = printResult(ts['data'], ts['label'], scores, ts['global_sliding_window'], ts['name'], modelName)\n",
    "    #L = [ '%.2f' % elem for elem in L]\n",
    "    #results.append([filename] + L)\n",
    "    results.append([filename] + L + [total_time, ts['X_data Length']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use for Dataset A\n",
    "columns = ['Filename'] + eval_metrics\n",
    "iforest_res = pd.DataFrame(results, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Name'] + ['AUC', 'Precision', 'Recall', 'F-score', 'Range-recall', 'ExistenceReward', 'OverlapReward', 'Range-precision', 'Range-Fscore', 'Precision@k', 'RangeAUC', 'Time', 'Number of Windows']\n",
    "df = pd.DataFrame(results, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Precision@k</th>\n",
       "      <th>Time</th>\n",
       "      <th>Number of Windows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ECG1</td>\n",
       "      <td>0.940874</td>\n",
       "      <td>0.230230</td>\n",
       "      <td>29.694501</td>\n",
       "      <td>229331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ECG1_20k</td>\n",
       "      <td>0.910233</td>\n",
       "      <td>0.265185</td>\n",
       "      <td>2.523816</td>\n",
       "      <td>19309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IOPS1</td>\n",
       "      <td>0.537252</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.843353</td>\n",
       "      <td>7349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SMD1</td>\n",
       "      <td>0.520366</td>\n",
       "      <td>0.063474</td>\n",
       "      <td>1.894012</td>\n",
       "      <td>28328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Occupancy1</td>\n",
       "      <td>0.645294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.875701</td>\n",
       "      <td>1756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ECG1+IOPS1</td>\n",
       "      <td>0.779123</td>\n",
       "      <td>0.114642</td>\n",
       "      <td>4.226586</td>\n",
       "      <td>28002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SMD1+Occupancy1</td>\n",
       "      <td>0.621276</td>\n",
       "      <td>0.038734</td>\n",
       "      <td>2.884676</td>\n",
       "      <td>30761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ECG1+IOPS1+Occupancy1</td>\n",
       "      <td>0.789963</td>\n",
       "      <td>0.082029</td>\n",
       "      <td>4.902659</td>\n",
       "      <td>30629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SMD1+ECG1+Occupancy1</td>\n",
       "      <td>0.776839</td>\n",
       "      <td>0.076019</td>\n",
       "      <td>6.240047</td>\n",
       "      <td>50572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ECG1+IOPS1+SMD1+Occupancy1</td>\n",
       "      <td>0.731821</td>\n",
       "      <td>0.056961</td>\n",
       "      <td>8.948484</td>\n",
       "      <td>59218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Name       AUC  Precision@k       Time  \\\n",
       "0                        ECG1  0.940874     0.230230  29.694501   \n",
       "1                    ECG1_20k  0.910233     0.265185   2.523816   \n",
       "2                       IOPS1  0.537252     0.000000   1.843353   \n",
       "3                        SMD1  0.520366     0.063474   1.894012   \n",
       "4                  Occupancy1  0.645294     0.000000   0.875701   \n",
       "5                  ECG1+IOPS1  0.779123     0.114642   4.226586   \n",
       "6             SMD1+Occupancy1  0.621276     0.038734   2.884676   \n",
       "7       ECG1+IOPS1+Occupancy1  0.789963     0.082029   4.902659   \n",
       "8        SMD1+ECG1+Occupancy1  0.776839     0.076019   6.240047   \n",
       "9  ECG1+IOPS1+SMD1+Occupancy1  0.731821     0.056961   8.948484   \n",
       "\n",
       "   Number of Windows  \n",
       "0             229331  \n",
       "1              19309  \n",
       "2               7349  \n",
       "3              28328  \n",
       "4               1756  \n",
       "5              28002  \n",
       "6              30761  \n",
       "7              30629  \n",
       "8              50572  \n",
       "9              59218  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Number of anomalies'] = df['Name'].apply(lambda x: np.sum(preprocessed_dict[x]['label']))\n",
    "df[['Name', 'AUC', 'Precision@k', 'Time', 'Number of Windows']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "                      Name &      AUC &  Precision@k &      Time &  Number of Windows \\\\\n",
      "\\midrule\n",
      "                      ECG1 & 0.940874 &     0.230230 & 29.694501 &             229331 \\\\\n",
      "                  ECG1\\_20k & 0.910233 &     0.265185 &  2.523816 &              19309 \\\\\n",
      "                     IOPS1 & 0.537252 &     0.000000 &  1.843353 &               7349 \\\\\n",
      "                      SMD1 & 0.520366 &     0.063474 &  1.894012 &              28328 \\\\\n",
      "                Occupancy1 & 0.645294 &     0.000000 &  0.875701 &               1756 \\\\\n",
      "                ECG1+IOPS1 & 0.779123 &     0.114642 &  4.226586 &              28002 \\\\\n",
      "           SMD1+Occupancy1 & 0.621276 &     0.038734 &  2.884676 &              30761 \\\\\n",
      "     ECG1+IOPS1+Occupancy1 & 0.789963 &     0.082029 &  4.902659 &              30629 \\\\\n",
      "      SMD1+ECG1+Occupancy1 & 0.776839 &     0.076019 &  6.240047 &              50572 \\\\\n",
      "ECG1+IOPS1+SMD1+Occupancy1 & 0.731821 &     0.056961 &  8.948484 &              59218 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df[['Name', 'AUC', 'Precision@k', 'Time', 'Number of Windows']].to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Only for Dataset A***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iforest_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "iforest_res.to_csv('Results/Isolation-Forest/IForest_Streaming_Naive_Variant.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "iforest_orig_res = pd.read_csv('Results/Isolation-Forest/IForest_Non-Streaming.csv')\n",
    "iforest_stream_var1_res = pd.read_csv('Results/Isolation-Forest/IForest_Streaming_Naive_Variant.csv')\n",
    "\n",
    "filenames_col = iforest_orig_res.iloc[:,0]\n",
    "\n",
    "iforest_orig_res = iforest_orig_res.iloc[:, 1:]\n",
    "iforest_stream_var1_res = iforest_stream_var1_res.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_diff =  iforest_stream_var1_res - iforest_orig_res\n",
    "\n",
    "res_diff.insert(0, 'Filename', filenames_col)\n",
    "\n",
    "res_diff = res_diff.style.applymap(highlight_diff, subset=pd.IndexSlice[:, res_diff.columns[1:]])\n",
    "\n",
    "res_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Isolation Forest***(Variant 2)\n",
    "Streaming variant with batch history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = 'IForest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for filename in preprocessed_dict.keys():\n",
    "    ts = preprocessed_dict[filename]\n",
    "    scores = []\n",
    "    previous_scores = None\n",
    "    x_data_partitions_len = 0\n",
    "    clf = IForest(n_jobs=7, random_state=42)\n",
    "    total_time = 0\n",
    "    \n",
    "    for par in range(n):\n",
    "\n",
    "        if par == 0:\n",
    "            partition = ts['data partitions'][par]\n",
    "            slidingWindow = find_length(partition)\n",
    "            X_train = Window(window=slidingWindow).convert(partition).to_numpy()\n",
    "        else:\n",
    "            partition_with_history = np.concatenate((ts['data partitions'][par-1], ts['data partitions'][par]))\n",
    "            slidingWindow = find_length(partition_with_history)\n",
    "            X_train = Window(window=slidingWindow).convert(partition_with_history).to_numpy()\n",
    "        \n",
    "        x_data_partitions_len += len(X_train)\n",
    "\n",
    "        t0 = time()\n",
    "        clf.fit(X_train)\n",
    "        \n",
    "\n",
    "        score = clf.decision_scores_\n",
    "\n",
    "        score = MinMaxScaler(feature_range=(0, 1)).fit_transform(score.reshape(-1, 1)).ravel()\n",
    "        score = np.array([score[0]] * math.ceil((slidingWindow-1)/2) +\n",
    "                         list(score) +\n",
    "                         [score[-1]] * ((slidingWindow-1)//2))\n",
    "\n",
    "        if par > 0:\n",
    "            previous_partition_length = len(ts['data partitions'][par-1])\n",
    "            new_previous_scores = score[:previous_partition_length]\n",
    "            mean_previous_scores = (previous_scores + new_previous_scores) / 2\n",
    "            scores[-previous_partition_length:] = mean_previous_scores.tolist()\n",
    "\n",
    "        current_partition_length = len(ts['data partitions'][par])\n",
    "        current_scores = score[-current_partition_length:]\n",
    "        scores.extend(current_scores)\n",
    "\n",
    "        previous_scores = current_scores\n",
    "\n",
    "        t1 = time()\n",
    "\n",
    "        total_time += t1 - t0\n",
    "    \n",
    "    \n",
    "    scores = np.array(scores)\n",
    "    # Plot figure\n",
    "    #plotFig(ts['data'], ts['label'], scores, ts['global_sliding_window'], fileName=ts['name'] + ' ' + loaded_dict[ts['name']][0], modelName=modelName)\n",
    "\n",
    "    # Calculate the results\n",
    "    L = printResult(ts['data'], ts['label'], scores, ts['global_sliding_window'], ts['name'], modelName)\n",
    "    #L = [ '%.2f' % elem for elem in L]\n",
    "    #results.append([filename] + L)\n",
    "    results.append([filename] + L + [total_time, x_data_partitions_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use for Dataset A\n",
    "columns = ['Filename'] + eval_metrics\n",
    "iforest_res = pd.DataFrame(results, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Name'] + ['AUC', 'Precision', 'Recall', 'F-score', 'Range-recall', 'ExistenceReward', 'OverlapReward', 'Range-precision', 'Range-Fscore', 'Precision@k', 'RangeAUC', 'Time', 'Number of Windows']\n",
    "df = pd.DataFrame(results, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Precision@k</th>\n",
       "      <th>Time</th>\n",
       "      <th>Number of Windows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ECG1</td>\n",
       "      <td>0.960342</td>\n",
       "      <td>0.223407</td>\n",
       "      <td>57.275025</td>\n",
       "      <td>413249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ECG1_20k</td>\n",
       "      <td>0.982224</td>\n",
       "      <td>0.515556</td>\n",
       "      <td>4.255884</td>\n",
       "      <td>35406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IOPS1</td>\n",
       "      <td>0.569987</td>\n",
       "      <td>0.009709</td>\n",
       "      <td>3.729669</td>\n",
       "      <td>14373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SMD1</td>\n",
       "      <td>0.713479</td>\n",
       "      <td>0.093541</td>\n",
       "      <td>4.671571</td>\n",
       "      <td>50872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Occupancy1</td>\n",
       "      <td>0.923740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.030245</td>\n",
       "      <td>4177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ECG1+IOPS1</td>\n",
       "      <td>0.796599</td>\n",
       "      <td>0.447219</td>\n",
       "      <td>7.624744</td>\n",
       "      <td>51116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SMD1+Occupancy1</td>\n",
       "      <td>0.770184</td>\n",
       "      <td>0.047190</td>\n",
       "      <td>6.253056</td>\n",
       "      <td>55555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ECG1+IOPS1+Occupancy1</td>\n",
       "      <td>0.888407</td>\n",
       "      <td>0.308689</td>\n",
       "      <td>8.716757</td>\n",
       "      <td>55888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SMD1+ECG1+Occupancy1</td>\n",
       "      <td>0.813415</td>\n",
       "      <td>0.218613</td>\n",
       "      <td>11.502622</td>\n",
       "      <td>91483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ECG1+IOPS1+SMD1+Occupancy1</td>\n",
       "      <td>0.781739</td>\n",
       "      <td>0.217506</td>\n",
       "      <td>18.793891</td>\n",
       "      <td>107065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Name       AUC  Precision@k       Time  \\\n",
       "0                        ECG1  0.960342     0.223407  57.275025   \n",
       "1                    ECG1_20k  0.982224     0.515556   4.255884   \n",
       "2                       IOPS1  0.569987     0.009709   3.729669   \n",
       "3                        SMD1  0.713479     0.093541   4.671571   \n",
       "4                  Occupancy1  0.923740     0.000000   1.030245   \n",
       "5                  ECG1+IOPS1  0.796599     0.447219   7.624744   \n",
       "6             SMD1+Occupancy1  0.770184     0.047190   6.253056   \n",
       "7       ECG1+IOPS1+Occupancy1  0.888407     0.308689   8.716757   \n",
       "8        SMD1+ECG1+Occupancy1  0.813415     0.218613  11.502622   \n",
       "9  ECG1+IOPS1+SMD1+Occupancy1  0.781739     0.217506  18.793891   \n",
       "\n",
       "   Number of Windows  \n",
       "0             413249  \n",
       "1              35406  \n",
       "2              14373  \n",
       "3              50872  \n",
       "4               4177  \n",
       "5              51116  \n",
       "6              55555  \n",
       "7              55888  \n",
       "8              91483  \n",
       "9             107065  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Number of anomalies'] = df['Name'].apply(lambda x: np.sum(preprocessed_dict[x]['label']))\n",
    "df[['Name', 'AUC', 'Precision@k', 'Time', 'Number of Windows']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "                      Name &      AUC &  Precision@k &      Time &  Number of Windows \\\\\n",
      "\\midrule\n",
      "                      ECG1 & 0.960342 &     0.223407 & 57.275025 &             413249 \\\\\n",
      "                  ECG1\\_20k & 0.982224 &     0.515556 &  4.255884 &              35406 \\\\\n",
      "                     IOPS1 & 0.569987 &     0.009709 &  3.729669 &              14373 \\\\\n",
      "                      SMD1 & 0.713479 &     0.093541 &  4.671571 &              50872 \\\\\n",
      "                Occupancy1 & 0.923740 &     0.000000 &  1.030245 &               4177 \\\\\n",
      "                ECG1+IOPS1 & 0.796599 &     0.447219 &  7.624744 &              51116 \\\\\n",
      "           SMD1+Occupancy1 & 0.770184 &     0.047190 &  6.253056 &              55555 \\\\\n",
      "     ECG1+IOPS1+Occupancy1 & 0.888407 &     0.308689 &  8.716757 &              55888 \\\\\n",
      "      SMD1+ECG1+Occupancy1 & 0.813415 &     0.218613 & 11.502622 &              91483 \\\\\n",
      "ECG1+IOPS1+SMD1+Occupancy1 & 0.781739 &     0.217506 & 18.793891 &             107065 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df[['Name', 'AUC', 'Precision@k', 'Time', 'Number of Windows']].to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Only for Dataset B***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iforest_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "iforest_res.to_csv('Results/Isolation-Forest/IForest_Streaming_Batch_History_Variant.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "iforest_orig_res = pd.read_csv('Results/Isolation-Forest/IForest_Non-Streaming.csv')\n",
    "iforest_stream_var1_res = pd.read_csv('Results/Isolation-Forest/IForest_Streaming_Naive_Variant.csv')\n",
    "iforest_stream_var2_res = pd.read_csv('Results/Isolation-Forest/IForest_Streaming_Batch_History_Variant.csv')\n",
    "\n",
    "filenames_col = iforest_orig_res.iloc[:,0]\n",
    "\n",
    "iforest_orig_res = iforest_orig_res.iloc[:, 1:]\n",
    "iforest_stream_var1_res = iforest_stream_var1_res.iloc[:, 1:]\n",
    "iforest_stream_var2_res = iforest_stream_var2_res.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_diff =  iforest_stream_var2_res - iforest_stream_var1_res\n",
    "\n",
    "res_diff.insert(0, 'Filename', filenames_col)\n",
    "\n",
    "res_diff = res_diff.style.applymap(highlight_diff, subset=pd.IndexSlice[:, res_diff.columns[1:]])\n",
    "\n",
    "res_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Isolation Forest***(Variant 3)\n",
    "Dynamic partitioning and classification based on ensemblers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = 'IForest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluates the last p points of the previous partition with both classifiers and replaces scores if they disagree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "disagreement_threshold = 0.5\n",
    "\n",
    "for filename in preprocessed_dict.keys():\n",
    "    ts = preprocessed_dict[filename]\n",
    "    scores = []\n",
    "    previous_scores = None\n",
    "    x_data_partitions_len = 0\n",
    "    clf = IForest(n_jobs=7, random_state=42)\n",
    "    total_time = 0\n",
    "\n",
    "    \n",
    "    for par in range(len(ts['data partitions'])):\n",
    "\n",
    "        if par == 0 or par == 1:\n",
    "            partition = ts['data partitions'][par]\n",
    "            slidingWindow = find_length(partition)\n",
    "            X_train = Window(window=slidingWindow).convert(partition).to_numpy()\n",
    "        else:\n",
    "            previous_partition = ts['data partitions'][par-1]\n",
    "            partition = ts['data partitions'][par]\n",
    "            last_p_points = previous_partition[-p:]\n",
    "            partition_with_history = np.concatenate((last_p_points, partition))\n",
    "            slidingWindow = find_length(partition_with_history)\n",
    "            X_train = Window(window=slidingWindow).convert(partition_with_history).to_numpy()\n",
    "        \n",
    "        x_data_partitions_len += len(X_train)\n",
    "\n",
    "        t0 = time()\n",
    "        clf.fit(X_train)\n",
    "        score = clf.decision_scores_\n",
    "\n",
    "        score = MinMaxScaler(feature_range=(0, 1)).fit_transform(score.reshape(-1, 1)).ravel()\n",
    "        score = np.array([score[0]] * math.ceil((slidingWindow-1)/2) +\n",
    "                        list(score) +\n",
    "                        [score[-1]] * ((slidingWindow-1)//2))\n",
    "        \n",
    "        if par > 1:\n",
    "            previous_partition_length = len(previous_partition)\n",
    "            new_previous_scores = score[:p]\n",
    "            prev_scores_to_compare = previous_scores[-p:]\n",
    "            \n",
    "            disagreement_indices = np.where(np.abs(prev_scores_to_compare - new_previous_scores) > disagreement_threshold)[0]\n",
    "            if len(disagreement_indices) > p * 0.5:\n",
    "                previous_scores[-p:] = new_previous_scores\n",
    "            \n",
    "            scores[-p:] = previous_scores[-p:]\n",
    "            current_scores = score[p:]\n",
    "        else:\n",
    "            current_scores = score\n",
    "\n",
    "        scores.extend(current_scores)\n",
    "        previous_scores = current_scores\n",
    "\n",
    "        t1 = time()\n",
    "        total_time += t1 - t0\n",
    "\n",
    "    \n",
    "    scores = np.array(scores)\n",
    "    # Plot figure\n",
    "    #plotFig(ts['data'], ts['label'], scores, ts['global_sliding_window'], fileName=ts['name'] + ' ' + loaded_dict[ts['name']][0], modelName=modelName)\n",
    "\n",
    "    # Calculate the results\n",
    "    L = printResult(ts['data'], ts['label'], scores, ts['global_sliding_window'], ts['name'], modelName)\n",
    "    #L = [ '%.2f' % elem for elem in L]\n",
    "    #results.append([filename] + L)\n",
    "    results.append([filename] + L + [total_time, x_data_partitions_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use for Dataset A\n",
    "columns = ['Filename'] + eval_metrics\n",
    "iforest_res = pd.DataFrame(results, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Name'] + ['AUC', 'Precision', 'Recall', 'F-score', 'Range-recall', 'ExistenceReward', 'OverlapReward', 'Range-precision', 'Range-Fscore', 'Precision@k', 'RangeAUC', 'Time', 'Number of Windows']\n",
    "df = pd.DataFrame(results, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Precision@k</th>\n",
       "      <th>Time</th>\n",
       "      <th>Number of Windows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ECG1</td>\n",
       "      <td>0.726683</td>\n",
       "      <td>0.007629</td>\n",
       "      <td>74.442923</td>\n",
       "      <td>342494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ECG1_20k</td>\n",
       "      <td>0.736483</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>6.702948</td>\n",
       "      <td>30106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IOPS1</td>\n",
       "      <td>0.527711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.221043</td>\n",
       "      <td>8210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SMD1</td>\n",
       "      <td>0.599773</td>\n",
       "      <td>0.122123</td>\n",
       "      <td>2.909586</td>\n",
       "      <td>29715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Occupancy1</td>\n",
       "      <td>0.861513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.485179</td>\n",
       "      <td>2417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ECG1+IOPS1</td>\n",
       "      <td>0.764026</td>\n",
       "      <td>0.044268</td>\n",
       "      <td>6.173537</td>\n",
       "      <td>37882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SMD1+Occupancy1</td>\n",
       "      <td>0.696195</td>\n",
       "      <td>0.125477</td>\n",
       "      <td>4.302852</td>\n",
       "      <td>32638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ECG1+IOPS1+Occupancy1</td>\n",
       "      <td>0.796491</td>\n",
       "      <td>0.093902</td>\n",
       "      <td>7.489961</td>\n",
       "      <td>40404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SMD1+ECG1+Occupancy1</td>\n",
       "      <td>0.626209</td>\n",
       "      <td>0.029256</td>\n",
       "      <td>12.268638</td>\n",
       "      <td>67581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ECG1+IOPS1+SMD1+Occupancy1</td>\n",
       "      <td>0.735903</td>\n",
       "      <td>0.096767</td>\n",
       "      <td>15.090683</td>\n",
       "      <td>63384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Name       AUC  Precision@k       Time  \\\n",
       "0                        ECG1  0.726683     0.007629  74.442923   \n",
       "1                    ECG1_20k  0.736483     0.007407   6.702948   \n",
       "2                       IOPS1  0.527711     0.000000   2.221043   \n",
       "3                        SMD1  0.599773     0.122123   2.909586   \n",
       "4                  Occupancy1  0.861513     0.000000   0.485179   \n",
       "5                  ECG1+IOPS1  0.764026     0.044268   6.173537   \n",
       "6             SMD1+Occupancy1  0.696195     0.125477   4.302852   \n",
       "7       ECG1+IOPS1+Occupancy1  0.796491     0.093902   7.489961   \n",
       "8        SMD1+ECG1+Occupancy1  0.626209     0.029256  12.268638   \n",
       "9  ECG1+IOPS1+SMD1+Occupancy1  0.735903     0.096767  15.090683   \n",
       "\n",
       "   Number of Windows  \n",
       "0             342494  \n",
       "1              30106  \n",
       "2               8210  \n",
       "3              29715  \n",
       "4               2417  \n",
       "5              37882  \n",
       "6              32638  \n",
       "7              40404  \n",
       "8              67581  \n",
       "9              63384  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Number of anomalies'] = df['Name'].apply(lambda x: np.sum(preprocessed_dict[x]['label']))\n",
    "df[['Name', 'AUC', 'Precision@k', 'Time', 'Number of Windows']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "                      Name &      AUC &  Precision@k &      Time &  Number of Windows \\\\\n",
      "\\midrule\n",
      "                      ECG1 & 0.726683 &     0.007629 & 74.442923 &             342494 \\\\\n",
      "                  ECG1\\_20k & 0.736483 &     0.007407 &  6.702948 &              30106 \\\\\n",
      "                     IOPS1 & 0.527711 &     0.000000 &  2.221043 &               8210 \\\\\n",
      "                      SMD1 & 0.599773 &     0.122123 &  2.909586 &              29715 \\\\\n",
      "                Occupancy1 & 0.861513 &     0.000000 &  0.485179 &               2417 \\\\\n",
      "                ECG1+IOPS1 & 0.764026 &     0.044268 &  6.173537 &              37882 \\\\\n",
      "           SMD1+Occupancy1 & 0.696195 &     0.125477 &  4.302852 &              32638 \\\\\n",
      "     ECG1+IOPS1+Occupancy1 & 0.796491 &     0.093902 &  7.489961 &              40404 \\\\\n",
      "      SMD1+ECG1+Occupancy1 & 0.626209 &     0.029256 & 12.268638 &              67581 \\\\\n",
      "ECG1+IOPS1+SMD1+Occupancy1 & 0.735903 &     0.096767 & 15.090683 &              63384 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df[['Name', 'AUC', 'Precision@k', 'Time', 'Number of Windows']].to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Only for Dataset A***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iforest_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "iforest_res.to_csv('Results/Isolation-Forest/IForest_Streaming_Dynamic_Partitioning_Variant.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "iforest_orig_res = pd.read_csv('Results/Isolation-Forest/IForest_Non-Streaming.csv')\n",
    "iforest_stream_var1_res = pd.read_csv('Results/Isolation-Forest/IForest_Streaming_Naive_Variant.csv')\n",
    "iforest_stream_var2_res = pd.read_csv('Results/Isolation-Forest/IForest_Streaming_Batch_History_Variant.csv')\n",
    "iforest_stream_var3_res = pd.read_csv('Results/Isolation-Forest/IForest_Streaming_Dynamic_Partitioning_Variant.csv')\n",
    "\n",
    "\n",
    "filenames_col = iforest_orig_res.iloc[:,0]\n",
    "\n",
    "iforest_orig_res = iforest_orig_res.iloc[:, 1:]\n",
    "iforest_stream_var1_res = iforest_stream_var1_res.iloc[:, 1:]\n",
    "iforest_stream_var2_res = iforest_stream_var2_res.iloc[:, 1:]\n",
    "iforest_stream_var3_res = iforest_stream_var3_res.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_diff =  iforest_stream_var3_res - iforest_stream_var2_res\n",
    "\n",
    "res_diff.insert(0, 'Filename', filenames_col)\n",
    "\n",
    "res_diff = res_diff.style.applymap(highlight_diff, subset=pd.IndexSlice[:, res_diff.columns[1:]])\n",
    "\n",
    "res_diff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
