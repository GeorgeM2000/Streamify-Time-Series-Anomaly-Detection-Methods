{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TSB_UAD.models.distance import Fourier\n",
    "from TSB_UAD.models.feature import Window\n",
    "from TSB_UAD.utils.slidingWindows import find_length, plotFig, printResult\n",
    "\n",
    "from TSB_UAD.models.iforest import IForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Isolation Forest***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Data Pre-Processing***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Dataset A***\n",
    "Dataset A utlizes time-series from the following domains:\n",
    "- Occupancy\n",
    "- SensorScope\n",
    "- NAB\n",
    "- NASA-MSL\n",
    "- SMD\n",
    "- YAHOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Time-Series dictionary\n",
    "with open('Time-Series_Data_Dictionaries/Time-Series-Random-Data-of-Interest-Dictionary.json', 'r') as json_file:\n",
    "    loaded_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some info about the generated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts1: ['Normality_1', 'SensorScope']\n",
      "ts3: ['Normality_1', 'NASA-MSL']\n",
      "ts4: ['Normality_1', 'YAHOO']\n",
      "ts5: ['Normality_1', 'SMD']\n",
      "ts8: ['Normality_1', 'SMD']\n",
      "ts2: ['Normality_2', 'SensorScope', 'NAB']\n",
      "ts9: ['Normality_2', 'Occupancy', 'NASA-MSL']\n",
      "ts6: ['Normality_3', 'SensorScope', 'YAHOO', 'NASA-MSL']\n",
      "ts7: ['Normality_3', 'YAHOO', 'NASA-MSL', 'SMD']\n"
     ]
    }
   ],
   "source": [
    "for filename, info in loaded_dict.items():\n",
    "    print(f'{filename}: {info}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Dataset B (Selected)***\n",
    "Dataset B utilizes the following time-series:\n",
    "- ECG1\n",
    "- ECG1_20k\n",
    "- IOPS1\n",
    "- SMD1\n",
    "- Occupancy1\n",
    "- ECG1+IOPS1\n",
    "- SMD1+Occupancy1 \n",
    "- ECG1+IOPS1+Occupancy1\n",
    "- SMD1+ECG1+Occupancy1\n",
    "- ECG1+IOPS1+SMD1+Occupancy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data for the evaluation.\n",
    "all_data = []\n",
    "\n",
    "with open('dataset.pkl', 'rb') as f:\n",
    "    data = pkl.load(f)\n",
    "\n",
    "all_data.extend(data['evaluation']['single_normality'])\n",
    "all_data.extend(data['evaluation']['double_normality'])\n",
    "all_data.extend(data['evaluation']['triple_normality'])\n",
    "all_data.extend(data['evaluation']['quadruple_normality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Pre-processing for non-streaming***\n",
    "Simple data pre-processing based on TSB-UAD. This pre-processing serves as the pre-processing baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for filename, info in loaded_dict.items():\n",
    "for timeseries in all_data:\n",
    "    #ts_filepath = f\"TS-Data-Files/{filename}\"\n",
    "    #ts = pd.read_csv(ts_filepath, header=None).dropna().to_numpy()\n",
    "\n",
    "    #name = ts_filepath.split('/')[-1]\n",
    "    #max_length = ts.shape[0]\n",
    "    #data = ts[:max_length, 0].astype(float)\n",
    "    #label = ts[:max_length, 1]\n",
    "\n",
    "    name = timeseries['Name']\n",
    "    data = timeseries['data']\n",
    "    max_length = data.shape[0]\n",
    "    label = timeseries['labels']\n",
    "\n",
    "    slidingWindow = find_length(data)\n",
    "    X_data = Window(window=slidingWindow).convert(data).to_numpy()\n",
    "\n",
    "    print(f'Time-Series name: {name}')\n",
    "    print(\"Estimated Subsequence length: \", slidingWindow)\n",
    "    print()\n",
    "    \n",
    "    preprocessed_dict[name] = {\n",
    "        'name': name,\n",
    "        'data': data,\n",
    "        'label': label,\n",
    "        'slidingWindow': slidingWindow,\n",
    "        'X_data': X_data,\n",
    "        'Time series length': len(data),\n",
    "        'Number of abnormal points': list(label).count(1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Pre-processing for both naive streaming variant and streaming variant with batch history***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-Series name: ECG1\n",
      "Estimated Subsequence length:  100\n",
      "\n",
      "Time-Series name: ECG1_20k\n",
      "Estimated Subsequence length:  100\n",
      "\n",
      "Time-Series name: IOPS1\n",
      "Estimated Subsequence length:  288\n",
      "\n",
      "Time-Series name: SMD1\n",
      "Estimated Subsequence length:  125\n",
      "\n",
      "Time-Series name: Occupancy1\n",
      "Estimated Subsequence length:  125\n",
      "\n",
      "Time-Series name: ECG1+IOPS1\n",
      "Estimated Subsequence length:  100\n",
      "\n",
      "Time-Series name: SMD1+Occupancy1\n",
      "Estimated Subsequence length:  125\n",
      "\n",
      "Time-Series name: ECG1+IOPS1+Occupancy1\n",
      "Estimated Subsequence length:  100\n",
      "\n",
      "Time-Series name: SMD1+ECG1+Occupancy1\n",
      "Estimated Subsequence length:  125\n",
      "\n",
      "Time-Series name: ECG1+IOPS1+SMD1+Occupancy1\n",
      "Estimated Subsequence length:  100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the number of windows to be fit per batch.\n",
    "windows_per_batch = 150\n",
    "\n",
    "for timeseries in all_data:\n",
    "    name = timeseries['Name']\n",
    "\n",
    "    data = timeseries['data']\n",
    "    max_length = data.shape[0]\n",
    "    label = timeseries['labels']\n",
    "\n",
    "    slidingWindow = find_length(data)\n",
    "    X_data = Window(window=slidingWindow).convert(data).to_numpy()\n",
    "\n",
    "    # Take the series and batch it.\n",
    "    batched_data = []\n",
    "\n",
    "    i = 0\n",
    "    flag = True\n",
    "    # Keep taking batches until the point at which no new windows can be taken.\n",
    "    while i < len(data) and flag:\n",
    "        # The data batches begin at the index indicated. If first batch, then the beginning of the time series.\n",
    "        batch_samples_begin = i\n",
    "\n",
    "        # The data batches end at the index where `windows_per_batch` can be *completely* extracted since the batch beginning. \n",
    "        # Formula: \n",
    "        #   i: current beginning of batch / offset\n",
    "        #   + slidingWindow: to have enough samples extract one window\n",
    "        #   + windows_per_batch: to have enough samples to extract the rest of the windows\n",
    "        #   - 1: because the first window extracted is counted twice\n",
    "        batch_samples_end = i + windows_per_batch + slidingWindow - 1\n",
    "        \n",
    "        # Guard against the ending of the time series where a full batch cannot be formed.\n",
    "        if batch_samples_end > len(data):\n",
    "            batch_samples_end = len(data)\n",
    "            flag = False\n",
    " \n",
    "        # Guard against case where the batch cannot hold even one window.\n",
    "        if len(data[batch_samples_begin:batch_samples_end]) < slidingWindow:\n",
    "            break\n",
    "\n",
    "        batched_data.append(data[batch_samples_begin:batch_samples_end])\n",
    "\n",
    "        # The next batch starts at the point where a new window be created after the last window of the last batch.\n",
    "        # So, end of the previous window - length of window = start of the last window.\n",
    "        #   start of the last window + 1 = start of the first window of the next batch.\n",
    "        i = batch_samples_end - slidingWindow + 1\n",
    "\n",
    "    # Take the windows and batch them.\n",
    "    batched_X_data = []\n",
    "    i = 0\n",
    "    while i < len(X_data):\n",
    "        begin = i\n",
    "        end = i + windows_per_batch\n",
    "        if end > len(X_data):\n",
    "            end = len(X_data)\n",
    "\n",
    "        batched_X_data.append(X_data[begin:end])\n",
    "        i += windows_per_batch\n",
    "\n",
    "    print(f'Time-Series name: {name}')\n",
    "    print(\"Estimated Subsequence length: \", slidingWindow)\n",
    "    print()\n",
    "    \n",
    "    # Store the pre-processed variables in the new dictionary\n",
    "    preprocessed_dict[name] = {\n",
    "        'name': name,\n",
    "        'data': data,\n",
    "        'label': label,\n",
    "        'slidingWindow': slidingWindow,\n",
    "        'X_data': X_data,\n",
    "        'batched_X_data': batched_X_data,\n",
    "        'batched_data': batched_data,\n",
    "        'Time series length': len(data),\n",
    "        'Number of abnormal points': list(label).count(1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Pre-processing for streaming variant with dynamic partitioning (change point detection)***\n",
    "Naively partitioning the data is not a reliable solution. We want to partition the data as soon as an abrupt change occurs. For that, we can use:\n",
    "- 1. MinMax range partitioning\n",
    "- 2. Percentile Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***MinMax range partitioning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for filename, info in loaded_dict.items():\n",
    "for timeseries in all_data:\n",
    "    #ts_filepath = f\"TS-Data-Files/{filename}\"\n",
    "    #ts = pd.read_csv(ts_filepath, header=None).dropna().to_numpy()\n",
    "\n",
    "    #name = ts_filepath.split('/')[-1]\n",
    "    #max_length = ts.shape[0]\n",
    "    #data = ts[:max_length, 0].astype(float)\n",
    "    #label = ts[:max_length, 1]\n",
    "\n",
    "    name = timeseries['Name']\n",
    "    data = timeseries['data']\n",
    "    max_length = data.shape[0]\n",
    "    label = timeseries['labels']\n",
    "    global_sw = find_length(data)\n",
    "\n",
    "    initial_partition_length = int(len(data) * 0.25)\n",
    "    initial_partition = data[:initial_partition_length]\n",
    "\n",
    "    max = np.max(initial_partition)\n",
    "    min = np.min(initial_partition)\n",
    "\n",
    "    data_partitions = [initial_partition]\n",
    "    current_partition = []\n",
    "    change_detected = False\n",
    "\n",
    "    p = 500\n",
    "    change_point_threshold = 0.8\n",
    "    exceed_threshold = 0.5\n",
    "    post_change_points = []\n",
    "\n",
    "    for point in data[len(initial_partition):]:\n",
    "        \n",
    "        # Check for significant change\n",
    "        if (point > max * (1 + change_point_threshold)) or (point < min * (1 - change_point_threshold)):\n",
    "            change_detected = True\n",
    "     \n",
    "        current_partition.append(point)\n",
    "\n",
    "\n",
    "        # After change, collect additional points\n",
    "        if change_detected:\n",
    "            post_change_points.append(point)\n",
    "            if len(post_change_points) == p:\n",
    "                exceeds_threshold_points = [(pt > max * (1 + change_point_threshold) or pt < min * (1 - change_point_threshold)) for pt in post_change_points]\n",
    "                if sum(exceeds_threshold_points) >= exceed_threshold * p:\n",
    "                    max = np.mean([max] + [pt for pt in post_change_points if pt > max])\n",
    "                    min = np.mean([min] + [pt for pt in post_change_points if pt < min])\n",
    "\n",
    "                post_change_points = []\n",
    "\n",
    "                # Add the current partition to data partitions\n",
    "                data_partitions.append(np.array(current_partition))\n",
    "                current_partition = []\n",
    "                change_detected = False\n",
    "                \n",
    "        \n",
    "    # Add any remaining points in current_partition to data_partitions\n",
    "    if current_partition:\n",
    "        data_partitions.append(np.array(current_partition))\n",
    "\n",
    "    \n",
    "    preprocessed_dict[name] = {\n",
    "        'name': name,\n",
    "        'data': data,\n",
    "        'label': label,\n",
    "        'data partitions': data_partitions,\n",
    "        'global_sliding_window': global_sw,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the number of partitions created for each time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 414 for file: ECG1\n",
      "Number of partitions: 32 for file: ECG1_20k\n",
      "Number of partitions: 3 for file: IOPS1\n",
      "Number of partitions: 2 for file: SMD1\n",
      "Number of partitions: 2 for file: Occupancy1\n",
      "Number of partitions: 33 for file: ECG1+IOPS1\n",
      "Number of partitions: 3 for file: SMD1+Occupancy1\n",
      "Number of partitions: 34 for file: ECG1+IOPS1+Occupancy1\n",
      "Number of partitions: 42 for file: SMD1+ECG1+Occupancy1\n",
      "Number of partitions: 34 for file: ECG1+IOPS1+SMD1+Occupancy1\n"
     ]
    }
   ],
   "source": [
    "for filename in preprocessed_dict.keys():\n",
    "    ts = preprocessed_dict[filename]\n",
    "\n",
    "    print(f\"Number of partitions: {len(ts['data partitions'])} for file: {ts['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the size of the partitions consistent with the initial size of the time-series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of partitions: 229900 for file: ECG1. Original data size: 229900\n",
      "Total size of partitions: 20000 for file: ECG1_20k. Original data size: 20000\n",
      "Total size of partitions: 8784 for file: IOPS1. Original data size: 8784\n",
      "Total size of partitions: 28479 for file: SMD1. Original data size: 28479\n",
      "Total size of partitions: 2665 for file: Occupancy1. Original data size: 2665\n",
      "Total size of partitions: 28784 for file: ECG1+IOPS1. Original data size: 28784\n",
      "Total size of partitions: 31144 for file: SMD1+Occupancy1. Original data size: 31144\n",
      "Total size of partitions: 31449 for file: ECG1+IOPS1+Occupancy1. Original data size: 31449\n",
      "Total size of partitions: 51144 for file: SMD1+ECG1+Occupancy1. Original data size: 51144\n",
      "Total size of partitions: 59928 for file: ECG1+IOPS1+SMD1+Occupancy1. Original data size: 59928\n"
     ]
    }
   ],
   "source": [
    "for filename in preprocessed_dict.keys():\n",
    "    ts = preprocessed_dict[filename]\n",
    "    par_size = 0\n",
    "    for partition in ts['data partitions']:\n",
    "        par_size += len(partition)\n",
    "    \n",
    "    print(f\"Total size of partitions: {par_size} for file: {ts['name']}. Original data size: {len(ts['data'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in preprocessed_dict.keys():\n",
    "    ts = preprocessed_dict[filename]\n",
    "\n",
    "    if len(ts['data partitions']) < 10:\n",
    "\n",
    "        fig, axes = plt.subplots(1, len(ts['data partitions']), figsize=(20, 5))\n",
    "\n",
    "        for i, array in enumerate(ts['data partitions']):\n",
    "            axes[i].plot(array)\n",
    "            axes[i].set_title(f\"Partition {i+1} of {ts['name']}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Percentile Partitioning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for filename, info in loaded_dict.items():\n",
    "for timeseries in all_data:\n",
    "    #ts_filepath = f\"TS-Data-Files/{filename}\"\n",
    "    #ts = pd.read_csv(ts_filepath, header=None).dropna().to_numpy()\n",
    "\n",
    "    #name = ts_filepath.split('/')[-1]\n",
    "    #max_length = ts.shape[0]\n",
    "    #data = ts[:max_length, 0].astype(float)\n",
    "    #label = ts[:max_length, 1]\n",
    "\n",
    "    name = timeseries['Name']\n",
    "    data = timeseries['data']\n",
    "    max_length = data.shape[0]\n",
    "    label = timeseries['labels']\n",
    "    global_sw = find_length(data)\n",
    "\n",
    "\n",
    "    # Filter the normal points (label == 0)\n",
    "    normal_indices = [i for i, lbl in enumerate(label) if lbl == 0]\n",
    "    normal_data = data[normal_indices]\n",
    "\n",
    "    normal_data_par_length = int(len(normal_data) * 0.10)\n",
    "    normal_data = normal_data[:normal_data_par_length]\n",
    "\n",
    "    #initial_partition_length = int(len(data) * 0.25)\n",
    "    #initial_partition = data[:initial_partition_length]\n",
    "\n",
    "    # Compute initial percentiles\n",
    "    percentile_5 = np.percentile(normal_data, 5)\n",
    "    percentile_95 = np.percentile(normal_data, 95)\n",
    "\n",
    "    data_partitions = []\n",
    "    current_partition = []\n",
    "    change_detected = False\n",
    "    p = 500\n",
    "    exceed_threshold = 0.5\n",
    "    post_change_points = []\n",
    "\n",
    "    for point in data[:]:\n",
    "        \n",
    "        # Check for significant change\n",
    "        if (point < percentile_5) or (point > percentile_95):\n",
    "            change_detected = True\n",
    "     \n",
    "        current_partition.append(point)\n",
    "\n",
    "\n",
    "        # After change, collect additional points\n",
    "        if change_detected:\n",
    "            post_change_points.append(point)\n",
    "            if len(post_change_points) == p:\n",
    "                exceeds_threshold_points = [(pt < percentile_5 or pt > percentile_95) for pt in post_change_points]\n",
    "                if sum(exceeds_threshold_points) / p >= exceed_threshold:\n",
    "                    # Update percentiles\n",
    "                    percentile_5 = np.percentile(post_change_points, 5)\n",
    "                    percentile_95 = np.percentile(post_change_points, 95)\n",
    "\n",
    "                post_change_points = []\n",
    "                # Add the current partition to data partitions\n",
    "                data_partitions.append(np.array(current_partition))\n",
    "                current_partition = []\n",
    "                change_detected = False\n",
    "                \n",
    "        \n",
    "    # Add any remaining points in current_partition to data_partitions\n",
    "    if current_partition:\n",
    "        data_partitions.append(np.array(current_partition))\n",
    "\n",
    "    \n",
    "    preprocessed_dict[name] = {\n",
    "        'name': name,\n",
    "        'data': data,\n",
    "        'label': label,\n",
    "        'data partitions': data_partitions,\n",
    "        'global_sliding_window': global_sw,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 431 for file: ECG1\n",
      "Number of partitions: 39 for file: ECG1_20k\n",
      "Number of partitions: 16 for file: IOPS1\n",
      "Number of partitions: 46 for file: SMD1\n",
      "Number of partitions: 4 for file: Occupancy1\n",
      "Number of partitions: 55 for file: ECG1+IOPS1\n",
      "Number of partitions: 51 for file: SMD1+Occupancy1\n",
      "Number of partitions: 58 for file: ECG1+IOPS1+Occupancy1\n",
      "Number of partitions: 89 for file: SMD1+ECG1+Occupancy1\n",
      "Number of partitions: 107 for file: ECG1+IOPS1+SMD1+Occupancy1\n"
     ]
    }
   ],
   "source": [
    "for filename in preprocessed_dict.keys():\n",
    "    ts = preprocessed_dict[filename]\n",
    "\n",
    "    print(f\"Number of partitions: {len(ts['data partitions'])} for file: {ts['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of partitions: 229900 for file: ECG1. Original data size: 229900\n",
      "Total size of partitions: 20000 for file: ECG1_20k. Original data size: 20000\n",
      "Total size of partitions: 8784 for file: IOPS1. Original data size: 8784\n",
      "Total size of partitions: 28479 for file: SMD1. Original data size: 28479\n",
      "Total size of partitions: 2665 for file: Occupancy1. Original data size: 2665\n",
      "Total size of partitions: 28784 for file: ECG1+IOPS1. Original data size: 28784\n",
      "Total size of partitions: 31144 for file: SMD1+Occupancy1. Original data size: 31144\n",
      "Total size of partitions: 31449 for file: ECG1+IOPS1+Occupancy1. Original data size: 31449\n",
      "Total size of partitions: 51144 for file: SMD1+ECG1+Occupancy1. Original data size: 51144\n",
      "Total size of partitions: 59928 for file: ECG1+IOPS1+SMD1+Occupancy1. Original data size: 59928\n"
     ]
    }
   ],
   "source": [
    "for filename in preprocessed_dict.keys():\n",
    "    ts = preprocessed_dict[filename]\n",
    "    par_size = 0\n",
    "    for partition in ts['data partitions']:\n",
    "        par_size += len(partition)\n",
    "    \n",
    "    print(f\"Total size of partitions: {par_size} for file: {ts['name']}. Original data size: {len(ts['data'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Plot TS length and number of abnormal points***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get filenames, time series lengths, and number of abnormal points\n",
    "filenames = list(preprocessed_dict.keys())\n",
    "time_series_lengths = [data['Time series length'] for data in preprocessed_dict.values()]\n",
    "number_of_abnormal_points = [data['Number of abnormal points'] for data in preprocessed_dict.values()]\n",
    "\n",
    "# Plot 'Time series length' and 'Number of abnormal points' for each filename\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(filenames, time_series_lengths, marker='o', linestyle='-', color='skyblue')\n",
    "plt.xlabel('Filename')\n",
    "plt.ylabel('Time series length')\n",
    "plt.title('Time Series Length for Each Filename')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(filenames, number_of_abnormal_points, marker='o', linestyle='-', color='lightgreen')\n",
    "plt.xlabel('Filename')\n",
    "plt.ylabel('Number of abnormal points')\n",
    "plt.title('Number of Abnormal Points for Each Filename')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Anomaly Detection***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Information:\n",
    "- TN: The point is normal and we predicted it is normal\n",
    "- TP: The point is abnormal and we predicted it is abnormal\n",
    "- FP: The point is normal and we predicted it is abnormal\n",
    "- FN: The point is abnormal and we predicted it is normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = ['AUC', \n",
    "                'Precision', \n",
    "                'Recall', \n",
    "                'F', \n",
    "                'Rrecall', \n",
    "                'ExistenceReward',\n",
    "                'OverlapReward',\n",
    "                'Rprecision',\n",
    "                'Rf',\n",
    "                'Precision@k',\n",
    "                'R_AUC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to colorize the cells of the dataframe results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_diff(val):\n",
    "    color = ''\n",
    "    if val > 0:\n",
    "        color = 'background-color: lightgreen'\n",
    "    elif val < 0:\n",
    "        color = 'background-color: lightcoral'\n",
    "    return color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Isolation Forest***(Original)\n",
    "Non-Streaming Variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = 'IForest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for filename in preprocessed_dict.keys():\n",
    "    ts = preprocessed_dict[filename]\n",
    "    x = ts['X_data']\n",
    "    clf = IForest(n_jobs=7, random_state=42)\n",
    "\n",
    "    t0 = time()\n",
    "    clf.fit(x)\n",
    "    \n",
    "    score = clf.decision_scores_\n",
    "    score = MinMaxScaler(feature_range=(0,1)).fit_transform(score.reshape(-1,1)).ravel()\n",
    "    score = np.array([score[0]]*math.ceil((ts['slidingWindow']-1)/2) + list(score) + [score[-1]]*((ts['slidingWindow']-1)//2))\n",
    "\n",
    "    t1 = time()\n",
    "    \n",
    "    # Plot figure\n",
    "    #plotFig(ts['data'], ts['label'], score, ts['slidingWindow'], fileName=ts['name'] + ' ' + loaded_dict[ts['name']][0], modelName=modelName)\n",
    "\n",
    "    # Calculate the results\n",
    "    L = printResult(ts['data'], ts['label'], score, ts['slidingWindow'], ts['name'], modelName)\n",
    "    #L = [ '%.2f' % elem for elem in L]\n",
    "    #results.append([filename] + L)\n",
    "    results.append([filename] + L + [t1-t0, len(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns = ['Filename'] + eval_metrics\n",
    "columns = ['Name'] + ['AUC', 'Precision', 'Recall', 'F-score', 'Range-recall', 'ExistenceReward', 'OverlapReward', 'Range-precision', 'Range-Fscore', 'Precision@k', 'RangeAUC', 'Time', 'Number of Windows']\n",
    "iforest_res = pd.DataFrame(results, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Precision@k</th>\n",
       "      <th>Time</th>\n",
       "      <th>Number of Windows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ECG1</td>\n",
       "      <td>0.963406</td>\n",
       "      <td>0.208339</td>\n",
       "      <td>28.327885</td>\n",
       "      <td>229801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ECG1_20k</td>\n",
       "      <td>0.973288</td>\n",
       "      <td>0.669630</td>\n",
       "      <td>2.175788</td>\n",
       "      <td>19901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IOPS1</td>\n",
       "      <td>0.534240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.343871</td>\n",
       "      <td>8497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SMD1</td>\n",
       "      <td>0.845381</td>\n",
       "      <td>0.306236</td>\n",
       "      <td>3.862060</td>\n",
       "      <td>28355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Occupancy1</td>\n",
       "      <td>0.871266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.331349</td>\n",
       "      <td>2541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ECG1+IOPS1</td>\n",
       "      <td>0.809130</td>\n",
       "      <td>0.533485</td>\n",
       "      <td>3.188019</td>\n",
       "      <td>28685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SMD1+Occupancy1</td>\n",
       "      <td>0.833035</td>\n",
       "      <td>0.223404</td>\n",
       "      <td>4.172662</td>\n",
       "      <td>31020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ECG1+IOPS1+Occupancy1</td>\n",
       "      <td>0.882892</td>\n",
       "      <td>0.462493</td>\n",
       "      <td>3.424442</td>\n",
       "      <td>31350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SMD1+ECG1+Occupancy1</td>\n",
       "      <td>0.688620</td>\n",
       "      <td>0.223912</td>\n",
       "      <td>6.872895</td>\n",
       "      <td>51020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ECG1+IOPS1+SMD1+Occupancy1</td>\n",
       "      <td>0.651722</td>\n",
       "      <td>0.213767</td>\n",
       "      <td>6.749730</td>\n",
       "      <td>59829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Name       AUC  Precision@k       Time  \\\n",
       "0                        ECG1  0.963406     0.208339  28.327885   \n",
       "1                    ECG1_20k  0.973288     0.669630   2.175788   \n",
       "2                       IOPS1  0.534240     0.000000   2.343871   \n",
       "3                        SMD1  0.845381     0.306236   3.862060   \n",
       "4                  Occupancy1  0.871266     0.000000   0.331349   \n",
       "5                  ECG1+IOPS1  0.809130     0.533485   3.188019   \n",
       "6             SMD1+Occupancy1  0.833035     0.223404   4.172662   \n",
       "7       ECG1+IOPS1+Occupancy1  0.882892     0.462493   3.424442   \n",
       "8        SMD1+ECG1+Occupancy1  0.688620     0.223912   6.872895   \n",
       "9  ECG1+IOPS1+SMD1+Occupancy1  0.651722     0.213767   6.749730   \n",
       "\n",
       "   Number of Windows  \n",
       "0             229801  \n",
       "1              19901  \n",
       "2               8497  \n",
       "3              28355  \n",
       "4               2541  \n",
       "5              28685  \n",
       "6              31020  \n",
       "7              31350  \n",
       "8              51020  \n",
       "9              59829  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iforest_res['Number of anomalies'] = iforest_res['Name'].apply(lambda x: np.sum(preprocessed_dict[x]['label']))\n",
    "iforest_res[['Name', 'AUC', 'Precision@k', 'Time', 'Number of Windows']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "                      Name &      AUC &  Precision@k &      Time &  Number of Windows \\\\\n",
      "\\midrule\n",
      "                      ECG1 & 0.963406 &     0.208339 & 28.327885 &             229801 \\\\\n",
      "                  ECG1\\_20k & 0.973288 &     0.669630 &  2.175788 &              19901 \\\\\n",
      "                     IOPS1 & 0.534240 &     0.000000 &  2.343871 &               8497 \\\\\n",
      "                      SMD1 & 0.845381 &     0.306236 &  3.862060 &              28355 \\\\\n",
      "                Occupancy1 & 0.871266 &     0.000000 &  0.331349 &               2541 \\\\\n",
      "                ECG1+IOPS1 & 0.809130 &     0.533485 &  3.188019 &              28685 \\\\\n",
      "           SMD1+Occupancy1 & 0.833035 &     0.223404 &  4.172662 &              31020 \\\\\n",
      "     ECG1+IOPS1+Occupancy1 & 0.882892 &     0.462493 &  3.424442 &              31350 \\\\\n",
      "      SMD1+ECG1+Occupancy1 & 0.688620 &     0.223912 &  6.872895 &              51020 \\\\\n",
      "ECG1+IOPS1+SMD1+Occupancy1 & 0.651722 &     0.213767 &  6.749730 &              59829 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(iforest_res[['Name', 'AUC', 'Precision@k', 'Time', 'Number of Windows']].to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iforest_res.to_csv('Results/Isolation-Forest/IForest_Non-Streaming.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Isolation Forest***(Variant 1)\n",
    "Naive Streaming Variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = 'IForest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for filename in preprocessed_dict.keys():\n",
    "    ts = preprocessed_dict[filename]\n",
    "    clf = IForest(n_jobs=7, random_state=42)\n",
    "    x = ts['X_data']\n",
    "    total_time = 0\n",
    "\n",
    "    score = []\n",
    "    #for par in range(n):\n",
    "    for batch in ts['batched_X_data']:\n",
    "\n",
    "        t0 = time()\n",
    "        if len(batch) == 1:\n",
    "            score.append(score[-1])\n",
    "        else:\n",
    "            clf.fit(batch)\n",
    "            score.extend(clf.decision_scores_)\n",
    "            t1 = time()\n",
    "\n",
    "            total_time += t1 - t0\n",
    "      \n",
    "    score = np.array(score)\n",
    "    score = MinMaxScaler(feature_range=(0,1)).fit_transform(score.reshape(-1,1)).ravel()\n",
    "    score = np.array([score[0]]*math.ceil((ts['slidingWindow']-1)/2) + list(score) + [score[-1]]*((ts['slidingWindow']-1)//2))\n",
    "\n",
    "    # Plot figure\n",
    "    #plotFig(ts['data'], ts['label'], scores, ts['global_sliding_window'], fileName=ts['name'] + ' ' + loaded_dict[ts['name']][0], modelName=modelName)\n",
    "\n",
    "    # Calculate the results\n",
    "    L = printResult(ts['data'], ts['label'], score, ts['slidingWindow'], ts['name'], modelName)\n",
    "    #L = [ '%.2f' % elem for elem in L]\n",
    "    #results.append([filename] + L)\n",
    "    results.append([filename] + L + [total_time, len(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns = ['Filename'] + eval_metrics\n",
    "columns = ['Name'] + ['AUC', 'Precision', 'Recall', 'F-score', 'Range-recall', 'ExistenceReward', 'OverlapReward', 'Range-precision', 'Range-Fscore', 'Precision@k', 'RangeAUC', 'Time', 'Number of Windows']\n",
    "iforest_res = pd.DataFrame(results, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Precision@k</th>\n",
       "      <th>Time</th>\n",
       "      <th>Number of Windows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ECG1</td>\n",
       "      <td>0.855992</td>\n",
       "      <td>0.142573</td>\n",
       "      <td>214.901253</td>\n",
       "      <td>229801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ECG1_20k</td>\n",
       "      <td>0.858151</td>\n",
       "      <td>0.314074</td>\n",
       "      <td>18.810870</td>\n",
       "      <td>19901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IOPS1</td>\n",
       "      <td>0.506936</td>\n",
       "      <td>0.004854</td>\n",
       "      <td>9.491467</td>\n",
       "      <td>8497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SMD1</td>\n",
       "      <td>0.367254</td>\n",
       "      <td>0.001856</td>\n",
       "      <td>29.045860</td>\n",
       "      <td>28355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Occupancy1</td>\n",
       "      <td>0.711537</td>\n",
       "      <td>0.064815</td>\n",
       "      <td>2.619673</td>\n",
       "      <td>2541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ECG1+IOPS1</td>\n",
       "      <td>0.675540</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.097246</td>\n",
       "      <td>28685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SMD1+Occupancy1</td>\n",
       "      <td>0.503673</td>\n",
       "      <td>0.031642</td>\n",
       "      <td>31.288422</td>\n",
       "      <td>31020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ECG1+IOPS1+Occupancy1</td>\n",
       "      <td>0.758577</td>\n",
       "      <td>0.031840</td>\n",
       "      <td>31.077644</td>\n",
       "      <td>31350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SMD1+ECG1+Occupancy1</td>\n",
       "      <td>0.645449</td>\n",
       "      <td>0.032020</td>\n",
       "      <td>51.184335</td>\n",
       "      <td>51020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ECG1+IOPS1+SMD1+Occupancy1</td>\n",
       "      <td>0.598401</td>\n",
       "      <td>0.016494</td>\n",
       "      <td>60.564411</td>\n",
       "      <td>59829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Name       AUC  Precision@k        Time  \\\n",
       "0                        ECG1  0.855992     0.142573  214.901253   \n",
       "1                    ECG1_20k  0.858151     0.314074   18.810870   \n",
       "2                       IOPS1  0.506936     0.004854    9.491467   \n",
       "3                        SMD1  0.367254     0.001856   29.045860   \n",
       "4                  Occupancy1  0.711537     0.064815    2.619673   \n",
       "5                  ECG1+IOPS1  0.675540     0.000000   29.097246   \n",
       "6             SMD1+Occupancy1  0.503673     0.031642   31.288422   \n",
       "7       ECG1+IOPS1+Occupancy1  0.758577     0.031840   31.077644   \n",
       "8        SMD1+ECG1+Occupancy1  0.645449     0.032020   51.184335   \n",
       "9  ECG1+IOPS1+SMD1+Occupancy1  0.598401     0.016494   60.564411   \n",
       "\n",
       "   Number of Windows  \n",
       "0             229801  \n",
       "1              19901  \n",
       "2               8497  \n",
       "3              28355  \n",
       "4               2541  \n",
       "5              28685  \n",
       "6              31020  \n",
       "7              31350  \n",
       "8              51020  \n",
       "9              59829  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iforest_res['Number of anomalies'] = iforest_res['Name'].apply(lambda x: np.sum(preprocessed_dict[x]['label']))\n",
    "iforest_res[['Name', 'AUC', 'Precision@k', 'Time', 'Number of Windows']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "                      Name &      AUC &  Precision@k &       Time &  Number of Windows \\\\\n",
      "\\midrule\n",
      "                      ECG1 & 0.855992 &     0.142573 & 214.901253 &             229801 \\\\\n",
      "                  ECG1\\_20k & 0.858151 &     0.314074 &  18.810870 &              19901 \\\\\n",
      "                     IOPS1 & 0.506936 &     0.004854 &   9.491467 &               8497 \\\\\n",
      "                      SMD1 & 0.367254 &     0.001856 &  29.045860 &              28355 \\\\\n",
      "                Occupancy1 & 0.711537 &     0.064815 &   2.619673 &               2541 \\\\\n",
      "                ECG1+IOPS1 & 0.675540 &     0.000000 &  29.097246 &              28685 \\\\\n",
      "           SMD1+Occupancy1 & 0.503673 &     0.031642 &  31.288422 &              31020 \\\\\n",
      "     ECG1+IOPS1+Occupancy1 & 0.758577 &     0.031840 &  31.077644 &              31350 \\\\\n",
      "      SMD1+ECG1+Occupancy1 & 0.645449 &     0.032020 &  51.184335 &              51020 \\\\\n",
      "ECG1+IOPS1+SMD1+Occupancy1 & 0.598401 &     0.016494 &  60.564411 &              59829 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(iforest_res[['Name', 'AUC', 'Precision@k', 'Time', 'Number of Windows']].to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Use only for Dataset A***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "iforest_res.to_csv('Results/Isolation-Forest/IForest_Streaming_Naive_Variant.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "iforest_orig_res = pd.read_csv('Results/Isolation-Forest/IForest_Non-Streaming.csv')\n",
    "iforest_stream_var1_res = pd.read_csv('Results/Isolation-Forest/IForest_Streaming_Naive_Variant.csv')\n",
    "\n",
    "filenames_col = iforest_orig_res.iloc[:,0]\n",
    "\n",
    "iforest_orig_res = iforest_orig_res.iloc[:, 1:]\n",
    "iforest_stream_var1_res = iforest_stream_var1_res.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_diff =  iforest_stream_var1_res - iforest_orig_res\n",
    "\n",
    "res_diff.insert(0, 'Filename', filenames_col)\n",
    "\n",
    "res_diff = res_diff.style.applymap(highlight_diff, subset=pd.IndexSlice[:, res_diff.columns[1:]])\n",
    "\n",
    "res_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Isolation Forest***(Variant 2)\n",
    "Streaming variant with batch history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = 'IForest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for filename in preprocessed_dict.keys():\n",
    "    ts = preprocessed_dict[filename]\n",
    "    scores = []\n",
    "    previous_scores = None\n",
    "    x = ts['X_data']\n",
    "    clf = IForest(n_jobs=7, random_state=42)\n",
    "    total_time = 0\n",
    "    \n",
    "    for i, _ in enumerate(ts['batched_X_data']):\n",
    "        \n",
    "        if i == 0:\n",
    "            X_train = ts['batched_X_data'][i]\n",
    "        else:\n",
    "            X_train = np.concatenate((ts['batched_X_data'][i-1], ts['batched_X_data'][i]))\n",
    "        \n",
    "        t0 = time()\n",
    "        clf.fit(X_train)\n",
    "        score = clf.decision_scores_\n",
    "\n",
    "        if i > 0:\n",
    "            previous_partition_length = len(ts['batched_X_data'][i-1])\n",
    "            new_previous_scores = score[:previous_partition_length]\n",
    "            mean_previous_scores = (previous_scores + new_previous_scores) / 2\n",
    "            scores[-previous_partition_length:] = mean_previous_scores.tolist()\n",
    "\n",
    "        current_partition_length = len(ts['batched_X_data'][i])\n",
    "        current_scores = score[-current_partition_length:]\n",
    "        scores.extend(current_scores)\n",
    "\n",
    "        previous_scores = current_scores\n",
    "\n",
    "        t1 = time()\n",
    "\n",
    "        total_time += t1 - t0\n",
    "    \n",
    "    \n",
    "    scores = np.array(scores)\n",
    "    scores = MinMaxScaler(feature_range=(0, 1)).fit_transform(scores.reshape(-1, 1)).ravel()\n",
    "    scores = np.array([scores[0]] * math.ceil((ts['slidingWindow']-1)/2) +\n",
    "                         list(scores) +\n",
    "                         [scores[-1]] * ((ts['slidingWindow']-1)//2))\n",
    "    \n",
    "\n",
    "    # Plot figure\n",
    "    #plotFig(ts['data'], ts['label'], scores, ts['global_sliding_window'], fileName=ts['name'] + ' ' + loaded_dict[ts['name']][0], modelName=modelName)\n",
    "\n",
    "    # Calculate the results\n",
    "    L = printResult(ts['data'], ts['label'], scores, ts['slidingWindow'], ts['name'], modelName)\n",
    "    #L = [ '%.2f' % elem for elem in L]\n",
    "    #results.append([filename] + L)\n",
    "    results.append([filename] + L + [total_time, len(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns = ['Filename'] + eval_metrics\n",
    "columns = ['Name'] + ['AUC', 'Precision', 'Recall', 'F-score', 'Range-recall', 'ExistenceReward', 'OverlapReward', 'Range-precision', 'Range-Fscore', 'Precision@k', 'RangeAUC', 'Time', 'Number of Windows']\n",
    "iforest_res = pd.DataFrame(results, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Precision@k</th>\n",
       "      <th>Time</th>\n",
       "      <th>Number of Windows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ECG1</td>\n",
       "      <td>0.832550</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>243.160621</td>\n",
       "      <td>229801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ECG1_20k</td>\n",
       "      <td>0.831822</td>\n",
       "      <td>0.371852</td>\n",
       "      <td>21.308003</td>\n",
       "      <td>19901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IOPS1</td>\n",
       "      <td>0.481980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.723807</td>\n",
       "      <td>8497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SMD1</td>\n",
       "      <td>0.320700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.594669</td>\n",
       "      <td>28355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Occupancy1</td>\n",
       "      <td>0.763412</td>\n",
       "      <td>0.065844</td>\n",
       "      <td>2.820671</td>\n",
       "      <td>2541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ECG1+IOPS1</td>\n",
       "      <td>0.668095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.753022</td>\n",
       "      <td>28685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SMD1+Occupancy1</td>\n",
       "      <td>0.478901</td>\n",
       "      <td>0.039825</td>\n",
       "      <td>34.556695</td>\n",
       "      <td>31020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ECG1+IOPS1+Occupancy1</td>\n",
       "      <td>0.761818</td>\n",
       "      <td>0.031301</td>\n",
       "      <td>33.960795</td>\n",
       "      <td>31350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SMD1+ECG1+Occupancy1</td>\n",
       "      <td>0.623556</td>\n",
       "      <td>0.041465</td>\n",
       "      <td>56.347982</td>\n",
       "      <td>51020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ECG1+IOPS1+SMD1+Occupancy1</td>\n",
       "      <td>0.574619</td>\n",
       "      <td>0.017594</td>\n",
       "      <td>64.708527</td>\n",
       "      <td>59829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Name       AUC  Precision@k        Time  \\\n",
       "0                        ECG1  0.832550     0.155556  243.160621   \n",
       "1                    ECG1_20k  0.831822     0.371852   21.308003   \n",
       "2                       IOPS1  0.481980     0.000000   10.723807   \n",
       "3                        SMD1  0.320700     0.000000   31.594669   \n",
       "4                  Occupancy1  0.763412     0.065844    2.820671   \n",
       "5                  ECG1+IOPS1  0.668095     0.000000   31.753022   \n",
       "6             SMD1+Occupancy1  0.478901     0.039825   34.556695   \n",
       "7       ECG1+IOPS1+Occupancy1  0.761818     0.031301   33.960795   \n",
       "8        SMD1+ECG1+Occupancy1  0.623556     0.041465   56.347982   \n",
       "9  ECG1+IOPS1+SMD1+Occupancy1  0.574619     0.017594   64.708527   \n",
       "\n",
       "   Number of Windows  \n",
       "0             229801  \n",
       "1              19901  \n",
       "2               8497  \n",
       "3              28355  \n",
       "4               2541  \n",
       "5              28685  \n",
       "6              31020  \n",
       "7              31350  \n",
       "8              51020  \n",
       "9              59829  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iforest_res['Number of anomalies'] = iforest_res['Name'].apply(lambda x: np.sum(preprocessed_dict[x]['label']))\n",
    "iforest_res[['Name', 'AUC', 'Precision@k', 'Time', 'Number of Windows']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "                      Name &      AUC &  Precision@k &       Time &  Number of Windows \\\\\n",
      "\\midrule\n",
      "                      ECG1 & 0.832550 &     0.155556 & 243.160621 &             229801 \\\\\n",
      "                  ECG1\\_20k & 0.831822 &     0.371852 &  21.308003 &              19901 \\\\\n",
      "                     IOPS1 & 0.481980 &     0.000000 &  10.723807 &               8497 \\\\\n",
      "                      SMD1 & 0.320700 &     0.000000 &  31.594669 &              28355 \\\\\n",
      "                Occupancy1 & 0.763412 &     0.065844 &   2.820671 &               2541 \\\\\n",
      "                ECG1+IOPS1 & 0.668095 &     0.000000 &  31.753022 &              28685 \\\\\n",
      "           SMD1+Occupancy1 & 0.478901 &     0.039825 &  34.556695 &              31020 \\\\\n",
      "     ECG1+IOPS1+Occupancy1 & 0.761818 &     0.031301 &  33.960795 &              31350 \\\\\n",
      "      SMD1+ECG1+Occupancy1 & 0.623556 &     0.041465 &  56.347982 &              51020 \\\\\n",
      "ECG1+IOPS1+SMD1+Occupancy1 & 0.574619 &     0.017594 &  64.708527 &              59829 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(iforest_res[['Name', 'AUC', 'Precision@k', 'Time', 'Number of Windows']].to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Use only for Dataset A***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "iforest_res.to_csv('Results/Isolation-Forest/IForest_Streaming_Batch_History_Variant.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "iforest_orig_res = pd.read_csv('Results/Isolation-Forest/IForest_Non-Streaming.csv')\n",
    "iforest_stream_var1_res = pd.read_csv('Results/Isolation-Forest/IForest_Streaming_Naive_Variant.csv')\n",
    "iforest_stream_var2_res = pd.read_csv('Results/Isolation-Forest/IForest_Streaming_Batch_History_Variant.csv')\n",
    "\n",
    "filenames_col = iforest_orig_res.iloc[:,0]\n",
    "\n",
    "iforest_orig_res = iforest_orig_res.iloc[:, 1:]\n",
    "iforest_stream_var1_res = iforest_stream_var1_res.iloc[:, 1:]\n",
    "iforest_stream_var2_res = iforest_stream_var2_res.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_diff =  iforest_stream_var2_res - iforest_stream_var1_res\n",
    "\n",
    "res_diff.insert(0, 'Filename', filenames_col)\n",
    "\n",
    "res_diff = res_diff.style.applymap(highlight_diff, subset=pd.IndexSlice[:, res_diff.columns[1:]])\n",
    "\n",
    "res_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Isolation Forest***(Variant 3)\n",
    "Dynamic partitioning and classification based on ensemblers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = 'IForest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluates the last p points of the previous partition with both classifiers and replaces scores if they disagree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "disagreement_threshold = 0.5\n",
    "\n",
    "for filename in preprocessed_dict.keys():\n",
    "    ts = preprocessed_dict[filename]\n",
    "    scores = []\n",
    "    previous_scores = None\n",
    "    x_data_partitions_len = 0\n",
    "    clf = IForest(n_jobs=7, random_state=42)\n",
    "    total_time = 0\n",
    "\n",
    "    for par in range(len(ts['data partitions'])):\n",
    "\n",
    "        if par == 0 or par == 1:\n",
    "            partition = ts['data partitions'][par]\n",
    "            slidingWindow = find_length(partition)\n",
    "            X_train = Window(window=slidingWindow).convert(partition).to_numpy()\n",
    "        else:\n",
    "            previous_partition = ts['data partitions'][par-1]\n",
    "            partition = ts['data partitions'][par]\n",
    "            last_p_points = previous_partition[-p:]\n",
    "            partition_with_history = np.concatenate((last_p_points, partition))\n",
    "            slidingWindow = find_length(partition_with_history)\n",
    "            X_train = Window(window=slidingWindow).convert(partition_with_history).to_numpy()\n",
    "        \n",
    "        x_data_partitions_len += len(X_train)\n",
    "\n",
    "        t0 = time()\n",
    "        clf.fit(X_train)\n",
    "        score = clf.decision_scores_\n",
    "\n",
    "        score = MinMaxScaler(feature_range=(0, 1)).fit_transform(score.reshape(-1, 1)).ravel()\n",
    "        score = np.array([score[0]] * math.ceil((slidingWindow-1)/2) +\n",
    "                        list(score) +\n",
    "                        [score[-1]] * ((slidingWindow-1)//2))\n",
    "        \n",
    "        if par > 1:\n",
    "            previous_partition_length = len(previous_partition)\n",
    "            new_previous_scores = score[:p]\n",
    "            prev_scores_to_compare = previous_scores[-p:]\n",
    "            \n",
    "            disagreement_indices = np.where(np.abs(prev_scores_to_compare - new_previous_scores) > disagreement_threshold)[0]\n",
    "            if len(disagreement_indices) > p * 0.5:\n",
    "                previous_scores[-p:] = new_previous_scores\n",
    "            \n",
    "            scores[-p:] = previous_scores[-p:]\n",
    "            current_scores = score[p:]\n",
    "        else:\n",
    "            current_scores = score\n",
    "\n",
    "        scores.extend(current_scores)\n",
    "        previous_scores = current_scores\n",
    "\n",
    "        t1 = time()\n",
    "        total_time += t1 - t0\n",
    "\n",
    "    \n",
    "    scores = np.array(scores)\n",
    "    # Plot figure\n",
    "    #plotFig(ts['data'], ts['label'], scores, ts['global_sliding_window'], fileName=ts['name'] + ' ' + loaded_dict[ts['name']][0], modelName=modelName)\n",
    "\n",
    "    # Calculate the results\n",
    "    L = printResult(ts['data'], ts['label'], scores, ts['global_sliding_window'], ts['name'], modelName)\n",
    "    #L = [ '%.2f' % elem for elem in L]\n",
    "    #results.append([filename] + L)\n",
    "    results.append([filename] + L + [total_time, x_data_partitions_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns = ['Filename'] + eval_metrics\n",
    "columns = ['Name'] + ['AUC', 'Precision', 'Recall', 'F-score', 'Range-recall', 'ExistenceReward', 'OverlapReward', 'Range-precision', 'Range-Fscore', 'Precision@k', 'RangeAUC', 'Time', 'Number of Windows']\n",
    "iforest_res = pd.DataFrame(results, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Precision@k</th>\n",
       "      <th>Time</th>\n",
       "      <th>Number of Windows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ECG1</td>\n",
       "      <td>0.643103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>92.059422</td>\n",
       "      <td>390369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ECG1_20k</td>\n",
       "      <td>0.680255</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>8.076657</td>\n",
       "      <td>33876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IOPS1</td>\n",
       "      <td>0.474765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.985141</td>\n",
       "      <td>11219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SMD1</td>\n",
       "      <td>0.498094</td>\n",
       "      <td>0.024499</td>\n",
       "      <td>8.856705</td>\n",
       "      <td>48238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Occupancy1</td>\n",
       "      <td>0.804776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.813956</td>\n",
       "      <td>3169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ECG1+IOPS1</td>\n",
       "      <td>0.590247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.332971</td>\n",
       "      <td>46367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SMD1+Occupancy1</td>\n",
       "      <td>0.595825</td>\n",
       "      <td>0.068740</td>\n",
       "      <td>9.912819</td>\n",
       "      <td>52664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ECG1+IOPS1+Occupancy1</td>\n",
       "      <td>0.689718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.827962</td>\n",
       "      <td>50322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SMD1+ECG1+Occupancy1</td>\n",
       "      <td>0.598437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.232967</td>\n",
       "      <td>86849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ECG1+IOPS1+SMD1+Occupancy1</td>\n",
       "      <td>0.549070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.739442</td>\n",
       "      <td>100196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Name       AUC  Precision@k       Time  \\\n",
       "0                        ECG1  0.643103     0.000000  92.059422   \n",
       "1                    ECG1_20k  0.680255     0.001481   8.076657   \n",
       "2                       IOPS1  0.474765     0.000000   3.985141   \n",
       "3                        SMD1  0.498094     0.024499   8.856705   \n",
       "4                  Occupancy1  0.804776     0.000000   0.813956   \n",
       "5                  ECG1+IOPS1  0.590247     0.000000  12.332971   \n",
       "6             SMD1+Occupancy1  0.595825     0.068740   9.912819   \n",
       "7       ECG1+IOPS1+Occupancy1  0.689718     0.000000  12.827962   \n",
       "8        SMD1+ECG1+Occupancy1  0.598437     0.000000  18.232967   \n",
       "9  ECG1+IOPS1+SMD1+Occupancy1  0.549070     0.000000  22.739442   \n",
       "\n",
       "   Number of Windows  \n",
       "0             390369  \n",
       "1              33876  \n",
       "2              11219  \n",
       "3              48238  \n",
       "4               3169  \n",
       "5              46367  \n",
       "6              52664  \n",
       "7              50322  \n",
       "8              86849  \n",
       "9             100196  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iforest_res['Number of anomalies'] = iforest_res['Name'].apply(lambda x: np.sum(preprocessed_dict[x]['label']))\n",
    "iforest_res[['Name', 'AUC', 'Precision@k', 'Time', 'Number of Windows']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "                      Name &      AUC &  Precision@k &      Time &  Number of Windows \\\\\n",
      "\\midrule\n",
      "                      ECG1 & 0.726683 &     0.007629 & 74.442923 &             342494 \\\\\n",
      "                  ECG1\\_20k & 0.736483 &     0.007407 &  6.702948 &              30106 \\\\\n",
      "                     IOPS1 & 0.527711 &     0.000000 &  2.221043 &               8210 \\\\\n",
      "                      SMD1 & 0.599773 &     0.122123 &  2.909586 &              29715 \\\\\n",
      "                Occupancy1 & 0.861513 &     0.000000 &  0.485179 &               2417 \\\\\n",
      "                ECG1+IOPS1 & 0.764026 &     0.044268 &  6.173537 &              37882 \\\\\n",
      "           SMD1+Occupancy1 & 0.696195 &     0.125477 &  4.302852 &              32638 \\\\\n",
      "     ECG1+IOPS1+Occupancy1 & 0.796491 &     0.093902 &  7.489961 &              40404 \\\\\n",
      "      SMD1+ECG1+Occupancy1 & 0.626209 &     0.029256 & 12.268638 &              67581 \\\\\n",
      "ECG1+IOPS1+SMD1+Occupancy1 & 0.735903 &     0.096767 & 15.090683 &              63384 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(iforest_res[['Name', 'AUC', 'Precision@k', 'Time', 'Number of Windows']].to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Use only for Dataset A***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "iforest_res.to_csv('Results/Isolation-Forest/IForest_Streaming_Dynamic_Partitioning_Variant.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "iforest_orig_res = pd.read_csv('Results/Isolation-Forest/IForest_Non-Streaming.csv')\n",
    "iforest_stream_var1_res = pd.read_csv('Results/Isolation-Forest/IForest_Streaming_Naive_Variant.csv')\n",
    "iforest_stream_var2_res = pd.read_csv('Results/Isolation-Forest/IForest_Streaming_Batch_History_Variant.csv')\n",
    "iforest_stream_var3_res = pd.read_csv('Results/Isolation-Forest/IForest_Streaming_Dynamic_Partitioning_Variant.csv')\n",
    "\n",
    "\n",
    "filenames_col = iforest_orig_res.iloc[:,0]\n",
    "\n",
    "iforest_orig_res = iforest_orig_res.iloc[:, 1:]\n",
    "iforest_stream_var1_res = iforest_stream_var1_res.iloc[:, 1:]\n",
    "iforest_stream_var2_res = iforest_stream_var2_res.iloc[:, 1:]\n",
    "iforest_stream_var3_res = iforest_stream_var3_res.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_diff =  iforest_stream_var3_res - iforest_stream_var2_res\n",
    "\n",
    "res_diff.insert(0, 'Filename', filenames_col)\n",
    "\n",
    "res_diff = res_diff.style.applymap(highlight_diff, subset=pd.IndexSlice[:, res_diff.columns[1:]])\n",
    "\n",
    "res_diff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
