{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TSB_UAD.models.distance import Fourier\n",
    "from TSB_UAD.models.feature import Window\n",
    "from TSB_UAD.utils.slidingWindows import find_length, plotFig, printResult\n",
    "\n",
    "from TSB_UAD.models.iforest import IForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Naive-Streaming Methods***\n",
    "TODO: Add comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Data Pre-Processing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Time-Series dictionary\n",
    "with open('Time-Series-Data-Dictionary.json', 'r') as json_file:\n",
    "    loaded_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts1: ['Normality_1', 'MGAB']\n",
      "ts2: ['Normality_2', 'SVDB', 'Dodgers']\n",
      "ts3: ['Normality_2', 'SMD', 'Dodgers']\n",
      "ts4: ['Normality_1', 'Daphnet']\n",
      "ts5: ['Normality_1', 'NAB']\n",
      "ts6: ['Normality_2', 'KDD21', 'GHL']\n",
      "ts7: ['Normality_3', 'MITDB', 'IOPS', 'Occupancy']\n",
      "ts8: ['Normality_1', 'ECG']\n",
      "ts9: ['Normality_1', 'NASA-MSL']\n",
      "ts10: ['Normality_2', 'Occupancy', 'NAB']\n",
      "ts11: ['Normality_3', 'SVDB', 'IOPS', 'Occupancy']\n",
      "ts12: ['Normality_3', 'Genesis', 'IOPS', 'SensorScope']\n",
      "ts13: ['Normality_3', 'Occupancy', 'Dodgers', 'SensorScope']\n",
      "ts14: ['Normality_2', 'GHL', 'SMD']\n",
      "ts15: ['Normality_3', 'SVDB', 'ECG', 'SensorScope']\n"
     ]
    }
   ],
   "source": [
    "for filename, info in loaded_dict.items():\n",
    "    print(f'{filename}: {info}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-Series filename: ts1\n",
      "Estimated Subsequence length:  125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from more_itertools import chunked\n",
    "\n",
    "# Set the number of windows to be fit per batch.\n",
    "windows_per_batch = 200\n",
    "\n",
    "for filename, info in loaded_dict.items():\n",
    "    ts_filepath = f\"TS-Data-Files/{filename}\"\n",
    "    \n",
    "    # === Pre-processing steps ===\n",
    "\n",
    "    # Prepare data for unsupervised method\n",
    "    ts = pd.read_csv(ts_filepath, header=None).dropna().to_numpy()\n",
    "\n",
    "    name = ts_filepath.split('/')[-1]\n",
    "    max_length = ts.shape[0]\n",
    "\n",
    "    data = ts[:max_length, 0].astype(float)\n",
    "    label = ts[:max_length, 1]\n",
    "\n",
    "    slidingWindow = find_length(data)\n",
    "    X_data = Window(window=slidingWindow).convert(data).to_numpy()\n",
    "\n",
    "    # Take the series and batch it.\n",
    "    batched_data = []\n",
    "\n",
    "    i = 0\n",
    "    flag = True\n",
    "    # Keep taking batches until the point at which no new windows can be taken.\n",
    "    while i < len(data) and flag:\n",
    "        # The data batches begin at the index indicated. If first batch, then the beginning of the time series.\n",
    "        batch_samples_begin = i\n",
    "\n",
    "        # The data batches end at the index where `windows_per_batch` can be *completely* extracted since the batch beginning. \n",
    "        # Formula: \n",
    "        #   i: current beginning of batch / offset\n",
    "        #   + slidingWindow: to have enough samples extract one window\n",
    "        #   + windows_per_batch: to have enough samples to extract the rest of the windows\n",
    "        #   - 1: because the first window extracted is counted twice\n",
    "        batch_samples_end = i + windows_per_batch + slidingWindow - 1\n",
    "        \n",
    "        # Guard against the ending of the time series where a full batch cannot be formed.\n",
    "        if batch_samples_end > len(data):\n",
    "            batch_samples_end = len(data)\n",
    "            flag = False\n",
    "\n",
    "        batched_data.append(data[batch_samples_begin:batch_samples_end])\n",
    "\n",
    "        # The next batch starts at the point where a new window be created after the last window of the last batch.\n",
    "        # So, end of the previous window - length of window = start of the last window.\n",
    "        #   start of the last window + 1 = start of the first window of the next batch.\n",
    "        i = batch_samples_end - slidingWindow + 1\n",
    "        #print(f'[Making Continuous Batches] i = {i}\\t len(data) = {len(data)}\\t Begin = {batch_samples_begin}\\t End = {batch_samples_end}\\t Batch Length = {len(data[batch_samples_begin:batch_samples_end])}\\t Batches Count = {len(batched_data)}')\n",
    "\n",
    "    # Take the windows and batch them.\n",
    "    batched_X_data = []\n",
    "    i = 0\n",
    "    while i < len(X_data):\n",
    "        begin = i\n",
    "        end = i + windows_per_batch\n",
    "        if end > len(X_data):\n",
    "            end = len(X_data)\n",
    "\n",
    "        batched_X_data.append(X_data[begin:end])\n",
    "        #print(f'[Making Batches of premade windows] i = {i}\\t len(X_data) = {len(X_data)}\\t Begin = {begin}\\t End = {end}\\t Batch Length = {len(X_data[begin:end])}\\t Batches Count = {len(batched_X_data)}')\n",
    "\n",
    "        i += windows_per_batch\n",
    "\n",
    "\n",
    "    # Prepare data for semisupervised method. \n",
    "    # Here, the training ratio = 0.1\n",
    "\n",
    "    data_train = data[:int(0.1 * len(data))]\n",
    "    data_test = data\n",
    "\n",
    "    X_train = Window(window=slidingWindow).convert(data_train).to_numpy()\n",
    "    X_test = Window(window=slidingWindow).convert(data_test).to_numpy()\n",
    "\n",
    "    print(f'Time-Series filename: {filename}')\n",
    "    print(\"Estimated Subsequence length: \", slidingWindow)\n",
    "    print()\n",
    "    \n",
    "    # Store the pre-processed variables in the new dictionary\n",
    "    preprocessed_dict[filename] = {\n",
    "        'name': name,\n",
    "        'data': data,\n",
    "        'batched_data': batched_data,\n",
    "        'label': label,\n",
    "        'slidingWindow': slidingWindow,\n",
    "        'X_data': X_data,\n",
    "        'batched_X_data': batched_X_data,\n",
    "        'data_train': data_train,\n",
    "        'data_test': data_test,\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'Time series length': len(data),\n",
    "        'Number of abnormal points': list(label).count(1)\n",
    "    }\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get filenames, time series lengths, and number of abnormal points\n",
    "filenames = list(preprocessed_dict.keys())\n",
    "time_series_lengths = [data['Time series length'] for data in preprocessed_dict.values()]\n",
    "number_of_abnormal_points = [data['Number of abnormal points'] for data in preprocessed_dict.values()]\n",
    "\n",
    "# Plot 'Time series length' and 'Number of abnormal points' for each filename\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(filenames, time_series_lengths, marker='o', linestyle='-', color='skyblue')\n",
    "plt.xlabel('Filename')\n",
    "plt.ylabel('Time series length')\n",
    "plt.title('Time Series Length for Each Filename')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(filenames, number_of_abnormal_points, marker='o', linestyle='-', color='lightgreen')\n",
    "plt.xlabel('Filename')\n",
    "plt.ylabel('Number of abnormal points')\n",
    "plt.title('Number of Abnormal Points for Each Filename')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Anomaly Detection***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Isolation Forest***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "modelName = 'IForest'\n",
    "clf = IForest(n_jobs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for filename in tqdm(preprocessed_dict.keys(), desc='Processing Time Series'):\n",
    "    ts = preprocessed_dict[filename]\n",
    "    x = ts['X_data']\n",
    "    \n",
    "    score = []\n",
    "    for batch in tqdm(ts['batched_X_data'], desc='Processing Batch'):\n",
    "        clf.fit(batch)\n",
    "        score.extend(clf.decision_scores_)\n",
    "    \n",
    "    score = np.array(score)\n",
    "    score = MinMaxScaler(feature_range=(0,1)).fit_transform(score.reshape(-1,1)).ravel()\n",
    "    score = np.array([score[0]]*math.ceil((ts['slidingWindow']-1)/2) + list(score) + [score[-1]]*((ts['slidingWindow']-1)//2))\n",
    "    \n",
    "    L = printResult(ts['data'], ts['label'], score, ts['slidingWindow'], ts['name'], modelName)\n",
    "    results.append([filename] + L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = ['Filename'] + [f'Metric_{i+1}' for i in range(len(results[0])-1)]\n",
    "columns = ['Filename'] + ['AUC', 'Precision', 'Recall', 'F-score', 'Range-recall', 'ExistenceReward', 'OverlapReward', 'Range-precision', 'Range-Fscore', 'Precison@k', 'RangeAUC']\n",
    "df = pd.DataFrame(results, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Results/Naive_Streaming/IsolationForest/results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***STUMP***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stumpy\n",
    "modelName = 'STUMP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for filename in tqdm(preprocessed_dict.keys(), desc='Processing Time Series'):\n",
    "    ts = preprocessed_dict[filename]\n",
    "    full_ts = ts['data']\n",
    "    subseries = ts['X_data']\n",
    "    window_size = ts['slidingWindow']\n",
    "    slidingWindow = window_size\n",
    "\n",
    "    k = 1\n",
    "    score = []\n",
    "    for batch in tqdm(ts['batched_data'], desc='Processing Batch'):\n",
    "        score_ = stumpy.stump(T_A=batch, m=window_size, k=k, ignore_trivial=True, normalize=True)\n",
    "    \n",
    "        score.extend(score_.T[k-1])\n",
    "        \n",
    "    score = np.array(score)\n",
    "    score = MinMaxScaler(feature_range=(0,1)).fit_transform(score.reshape(-1,1)).ravel()\n",
    "    score = np.array([score[0]]*math.ceil((slidingWindow-1)/2) + list(score) + [score[-1]]*((slidingWindow-1)//2))\n",
    "    \n",
    "    L = printResult(ts['data'], ts['label'], score, ts['slidingWindow'], ts['name'], modelName)\n",
    "    results.append([filename] + L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Filename'] + ['AUC', 'Precision', 'Recall', 'F-score', 'Range-recall', 'ExistenceReward', 'OverlapReward', 'Range-precision', 'Range-Fscore', 'Precison@k', 'RangeAUC']\n",
    "df = pd.DataFrame(results, columns=columns)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
