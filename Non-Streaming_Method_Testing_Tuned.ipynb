{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TSB_UAD.models.distance import Fourier\n",
    "from TSB_UAD.models.feature import Window\n",
    "from TSB_UAD.utils.slidingWindows import find_length, plotFig, printResult\n",
    "\n",
    "from TSB_UAD.models.iforest import IForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Isolation Forest***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Data Pre-Processing***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Dataset A***\n",
    "Dataset A utlizes time-series from the following domains:\n",
    "- Occupancy\n",
    "- SensorScope\n",
    "- NAB\n",
    "- NASA-MSL\n",
    "- SMD\n",
    "- YAHOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Time-Series dictionary\n",
    "with open('Time-Series_Data_Dictionaries/Time-Series-Random-Data-of-Interest-Dictionary.json', 'r') as json_file:\n",
    "    loaded_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some info about the generated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts1: ['Normality_1', 'SensorScope']\n",
      "ts3: ['Normality_1', 'NASA-MSL']\n",
      "ts4: ['Normality_1', 'YAHOO']\n",
      "ts5: ['Normality_1', 'SMD']\n",
      "ts8: ['Normality_1', 'SMD']\n",
      "ts2: ['Normality_2', 'SensorScope', 'NAB']\n",
      "ts9: ['Normality_2', 'Occupancy', 'NASA-MSL']\n",
      "ts6: ['Normality_3', 'SensorScope', 'YAHOO', 'NASA-MSL']\n",
      "ts7: ['Normality_3', 'YAHOO', 'NASA-MSL', 'SMD']\n"
     ]
    }
   ],
   "source": [
    "for filename, info in loaded_dict.items():\n",
    "    print(f'{filename}: {info}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Dataset B (Selected)***\n",
    "Dataset B utilizes the following time-series:\n",
    "- ECG1\n",
    "- ECG1_20k\n",
    "- IOPS1\n",
    "- SMD1\n",
    "- Occupancy1\n",
    "- ECG1+IOPS1\n",
    "- SMD1+Occupancy1 \n",
    "- ECG1+IOPS1+Occupancy1\n",
    "- SMD1+ECG1+Occupancy1\n",
    "- ECG1+IOPS1+SMD1+Occupancy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data for the evaluation.\n",
    "all_data = []\n",
    "\n",
    "with open('dataset.pkl', 'rb') as f:\n",
    "    data = pkl.load(f)\n",
    "\n",
    "all_data.extend(data['evaluation']['single_normality'])\n",
    "all_data.extend(data['evaluation']['double_normality'])\n",
    "all_data.extend(data['evaluation']['triple_normality'])\n",
    "all_data.extend(data['evaluation']['quadruple_normality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Pre-processing for non-streaming***\n",
    "Simple data pre-processing based on TSB-UAD. This pre-processing serves as the pre-processing baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for filename, info in loaded_dict.items():\n",
    "for timeseries in all_data:\n",
    "    #ts_filepath = f\"TS-Data-Files/{filename}\"\n",
    "    #ts = pd.read_csv(ts_filepath, header=None).dropna().to_numpy()\n",
    "\n",
    "    #name = ts_filepath.split('/')[-1]\n",
    "    #max_length = ts.shape[0]\n",
    "    #data = ts[:max_length, 0].astype(float)\n",
    "    #label = ts[:max_length, 1]\n",
    "\n",
    "    name = timeseries['Name']\n",
    "    data = timeseries['data']\n",
    "    max_length = data.shape[0]\n",
    "    label = timeseries['labels']\n",
    "\n",
    "    slidingWindow = find_length(data)\n",
    "    X_data = Window(window=slidingWindow).convert(data).to_numpy()\n",
    "\n",
    "    print(f'Time-Series name: {name}')\n",
    "    print(\"Estimated Subsequence length: \", slidingWindow)\n",
    "    print()\n",
    "    \n",
    "    preprocessed_dict[name] = {\n",
    "        'name': name,\n",
    "        'data': data,\n",
    "        'label': label,\n",
    "        'slidingWindow': slidingWindow,\n",
    "        'X_data': X_data,\n",
    "        'Time series length': len(data),\n",
    "        'Number of abnormal points': list(label).count(1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Pre-processing for both naive streaming variant and streaming variant with batch history***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-Series name: ECG1\n",
      "Estimated Subsequence length:  100\n",
      "\n",
      "Time-Series name: ECG1_20k\n",
      "Estimated Subsequence length:  100\n",
      "\n",
      "Time-Series name: IOPS1\n",
      "Estimated Subsequence length:  288\n",
      "\n",
      "Time-Series name: SMD1\n",
      "Estimated Subsequence length:  125\n",
      "\n",
      "Time-Series name: Occupancy1\n",
      "Estimated Subsequence length:  125\n",
      "\n",
      "Time-Series name: ECG1+IOPS1\n",
      "Estimated Subsequence length:  100\n",
      "\n",
      "Time-Series name: SMD1+Occupancy1\n",
      "Estimated Subsequence length:  125\n",
      "\n",
      "Time-Series name: ECG1+IOPS1+Occupancy1\n",
      "Estimated Subsequence length:  100\n",
      "\n",
      "Time-Series name: SMD1+ECG1+Occupancy1\n",
      "Estimated Subsequence length:  125\n",
      "\n",
      "Time-Series name: ECG1+IOPS1+SMD1+Occupancy1\n",
      "Estimated Subsequence length:  100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the number of windows to be fit per batch.\n",
    "windows_per_batch = 150\n",
    "\n",
    "for timeseries in all_data:\n",
    "    name = timeseries['Name']\n",
    "\n",
    "    data = timeseries['data']\n",
    "    max_length = data.shape[0]\n",
    "    label = timeseries['labels']\n",
    "\n",
    "    slidingWindow = find_length(data)\n",
    "    X_data = Window(window=slidingWindow).convert(data).to_numpy()\n",
    "\n",
    "    # Take the series and batch it.\n",
    "    batched_data = []\n",
    "\n",
    "    i = 0\n",
    "    flag = True\n",
    "    # Keep taking batches until the point at which no new windows can be taken.\n",
    "    while i < len(data) and flag:\n",
    "        # The data batches begin at the index indicated. If first batch, then the beginning of the time series.\n",
    "        batch_samples_begin = i\n",
    "\n",
    "        # The data batches end at the index where `windows_per_batch` can be *completely* extracted since the batch beginning. \n",
    "        # Formula: \n",
    "        #   i: current beginning of batch / offset\n",
    "        #   + slidingWindow: to have enough samples extract one window\n",
    "        #   + windows_per_batch: to have enough samples to extract the rest of the windows\n",
    "        #   - 1: because the first window extracted is counted twice\n",
    "        batch_samples_end = i + windows_per_batch + slidingWindow - 1\n",
    "        \n",
    "        # Guard against the ending of the time series where a full batch cannot be formed.\n",
    "        if batch_samples_end > len(data):\n",
    "            batch_samples_end = len(data)\n",
    "            flag = False\n",
    " \n",
    "        # Guard against case where the batch cannot hold even one window.\n",
    "        if len(data[batch_samples_begin:batch_samples_end]) < slidingWindow:\n",
    "            break\n",
    "\n",
    "        batched_data.append(data[batch_samples_begin:batch_samples_end])\n",
    "\n",
    "        # The next batch starts at the point where a new window be created after the last window of the last batch.\n",
    "        # So, end of the previous window - length of window = start of the last window.\n",
    "        #   start of the last window + 1 = start of the first window of the next batch.\n",
    "        i = batch_samples_end - slidingWindow + 1\n",
    "\n",
    "    # Take the windows and batch them.\n",
    "    batched_X_data = []\n",
    "    i = 0\n",
    "    while i < len(X_data):\n",
    "        begin = i\n",
    "        end = i + windows_per_batch\n",
    "        if end > len(X_data):\n",
    "            end = len(X_data)\n",
    "\n",
    "        batched_X_data.append(X_data[begin:end])\n",
    "        i += windows_per_batch\n",
    "\n",
    "    print(f'Time-Series name: {name}')\n",
    "    print(\"Estimated Subsequence length: \", slidingWindow)\n",
    "    print()\n",
    "    \n",
    "    # Store the pre-processed variables in the new dictionary\n",
    "    preprocessed_dict[name] = {\n",
    "        'name': name,\n",
    "        'data': data,\n",
    "        'label': label,\n",
    "        'slidingWindow': slidingWindow,\n",
    "        'X_data': X_data,\n",
    "        'batched_X_data': batched_X_data,\n",
    "        'batched_data': batched_data,\n",
    "        'Time series length': len(data),\n",
    "        'Number of abnormal points': list(label).count(1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Pre-processing for streaming variant with dynamic partitioning (change point detection)***\n",
    "Naively partitioning the data is not a reliable solution. We want to partition the data as soon as an abrupt change occurs. For that, we can use:\n",
    "- 1. MinMax range partitioning\n",
    "- 2. Percentile Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***MinMax range partitioning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for filename, info in loaded_dict.items():\n",
    "for timeseries in all_data:\n",
    "    #ts_filepath = f\"TS-Data-Files/{filename}\"\n",
    "    #ts = pd.read_csv(ts_filepath, header=None).dropna().to_numpy()\n",
    "\n",
    "    #name = ts_filepath.split('/')[-1]\n",
    "    #max_length = ts.shape[0]\n",
    "    #data = ts[:max_length, 0].astype(float)\n",
    "    #label = ts[:max_length, 1]\n",
    "\n",
    "    name = timeseries['Name']\n",
    "    data = timeseries['data']\n",
    "    max_length = data.shape[0]\n",
    "    label = timeseries['labels']\n",
    "    global_sw = find_length(data)\n",
    "\n",
    "    initial_partition_length = int(len(data) * 0.25)\n",
    "    initial_partition = data[:initial_partition_length]\n",
    "\n",
    "    max = np.max(initial_partition)\n",
    "    min = np.min(initial_partition)\n",
    "\n",
    "    data_partitions = [initial_partition]\n",
    "    current_partition = []\n",
    "    change_detected = False\n",
    "\n",
    "    p = 500\n",
    "    change_point_threshold = 0.8\n",
    "    exceed_threshold = 0.5\n",
    "    post_change_points = []\n",
    "\n",
    "    for point in data[len(initial_partition):]:\n",
    "        \n",
    "        # Check for significant change\n",
    "        if (point > max * (1 + change_point_threshold)) or (point < min * (1 - change_point_threshold)):\n",
    "            change_detected = True\n",
    "     \n",
    "        current_partition.append(point)\n",
    "\n",
    "\n",
    "        # After change, collect additional points\n",
    "        if change_detected:\n",
    "            post_change_points.append(point)\n",
    "            if len(post_change_points) == p:\n",
    "                exceeds_threshold_points = [(pt > max * (1 + change_point_threshold) or pt < min * (1 - change_point_threshold)) for pt in post_change_points]\n",
    "                if sum(exceeds_threshold_points) >= exceed_threshold * p:\n",
    "                    max = np.mean([max] + [pt for pt in post_change_points if pt > max])\n",
    "                    min = np.mean([min] + [pt for pt in post_change_points if pt < min])\n",
    "\n",
    "                post_change_points = []\n",
    "\n",
    "                # Add the current partition to data partitions\n",
    "                data_partitions.append(np.array(current_partition))\n",
    "                current_partition = []\n",
    "                change_detected = False\n",
    "                \n",
    "        \n",
    "    # Add any remaining points in current_partition to data_partitions\n",
    "    if current_partition:\n",
    "        data_partitions.append(np.array(current_partition))\n",
    "\n",
    "    \n",
    "    preprocessed_dict[name] = {\n",
    "        'name': name,\n",
    "        'data': data,\n",
    "        'label': label,\n",
    "        'data partitions': data_partitions,\n",
    "        'global_sliding_window': global_sw,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the number of partitions created for each time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 414 for file: ECG1\n",
      "Number of partitions: 32 for file: ECG1_20k\n",
      "Number of partitions: 3 for file: IOPS1\n",
      "Number of partitions: 2 for file: SMD1\n",
      "Number of partitions: 2 for file: Occupancy1\n",
      "Number of partitions: 33 for file: ECG1+IOPS1\n",
      "Number of partitions: 3 for file: SMD1+Occupancy1\n",
      "Number of partitions: 34 for file: ECG1+IOPS1+Occupancy1\n",
      "Number of partitions: 42 for file: SMD1+ECG1+Occupancy1\n",
      "Number of partitions: 34 for file: ECG1+IOPS1+SMD1+Occupancy1\n"
     ]
    }
   ],
   "source": [
    "for filename in preprocessed_dict.keys():\n",
    "    ts = preprocessed_dict[filename]\n",
    "\n",
    "    print(f\"Number of partitions: {len(ts['data partitions'])} for file: {ts['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the size of the partitions consistent with the initial size of the time-series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of partitions: 229900 for file: ECG1. Original data size: 229900\n",
      "Total size of partitions: 20000 for file: ECG1_20k. Original data size: 20000\n",
      "Total size of partitions: 8784 for file: IOPS1. Original data size: 8784\n",
      "Total size of partitions: 28479 for file: SMD1. Original data size: 28479\n",
      "Total size of partitions: 2665 for file: Occupancy1. Original data size: 2665\n",
      "Total size of partitions: 28784 for file: ECG1+IOPS1. Original data size: 28784\n",
      "Total size of partitions: 31144 for file: SMD1+Occupancy1. Original data size: 31144\n",
      "Total size of partitions: 31449 for file: ECG1+IOPS1+Occupancy1. Original data size: 31449\n",
      "Total size of partitions: 51144 for file: SMD1+ECG1+Occupancy1. Original data size: 51144\n",
      "Total size of partitions: 59928 for file: ECG1+IOPS1+SMD1+Occupancy1. Original data size: 59928\n"
     ]
    }
   ],
   "source": [
    "for filename in preprocessed_dict.keys():\n",
    "    ts = preprocessed_dict[filename]\n",
    "    par_size = 0\n",
    "    for partition in ts['data partitions']:\n",
    "        par_size += len(partition)\n",
    "    \n",
    "    print(f\"Total size of partitions: {par_size} for file: {ts['name']}. Original data size: {len(ts['data'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in preprocessed_dict.keys():\n",
    "    ts = preprocessed_dict[filename]\n",
    "\n",
    "    if len(ts['data partitions']) < 10:\n",
    "\n",
    "        fig, axes = plt.subplots(1, len(ts['data partitions']), figsize=(20, 5))\n",
    "\n",
    "        for i, array in enumerate(ts['data partitions']):\n",
    "            axes[i].plot(array)\n",
    "            axes[i].set_title(f\"Partition {i+1} of {ts['name']}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Percentile Partitioning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for filename, info in loaded_dict.items():\n",
    "for timeseries in all_data:\n",
    "    #ts_filepath = f\"TS-Data-Files/{filename}\"\n",
    "    #ts = pd.read_csv(ts_filepath, header=None).dropna().to_numpy()\n",
    "\n",
    "    #name = ts_filepath.split('/')[-1]\n",
    "    #max_length = ts.shape[0]\n",
    "    #data = ts[:max_length, 0].astype(float)\n",
    "    #label = ts[:max_length, 1]\n",
    "\n",
    "    name = timeseries['Name']\n",
    "    data = timeseries['data']\n",
    "    max_length = data.shape[0]\n",
    "    label = timeseries['labels']\n",
    "    global_sw = find_length(data)\n",
    "\n",
    "\n",
    "    # Filter the normal points (label == 0)\n",
    "    normal_indices = [i for i, lbl in enumerate(label) if lbl == 0]\n",
    "    normal_data = data[normal_indices]\n",
    "\n",
    "    normal_data_par_length = int(len(normal_data) * 0.10)\n",
    "    normal_data = normal_data[:normal_data_par_length]\n",
    "\n",
    "    #initial_partition_length = int(len(data) * 0.25)\n",
    "    #initial_partition = data[:initial_partition_length]\n",
    "\n",
    "    # Compute initial percentiles\n",
    "    percentile_5 = np.percentile(normal_data, 5)\n",
    "    percentile_95 = np.percentile(normal_data, 95)\n",
    "\n",
    "    data_partitions = []\n",
    "    current_partition = []\n",
    "    change_detected = False\n",
    "    p = 500\n",
    "    exceed_threshold = 0.5\n",
    "    post_change_points = []\n",
    "\n",
    "    for point in data[:]:\n",
    "        \n",
    "        # Check for significant change\n",
    "        if (point < percentile_5) or (point > percentile_95):\n",
    "            change_detected = True\n",
    "     \n",
    "        current_partition.append(point)\n",
    "\n",
    "\n",
    "        # After change, collect additional points\n",
    "        if change_detected:\n",
    "            post_change_points.append(point)\n",
    "            if len(post_change_points) == p:\n",
    "                exceeds_threshold_points = [(pt < percentile_5 or pt > percentile_95) for pt in post_change_points]\n",
    "                if sum(exceeds_threshold_points) / p >= exceed_threshold:\n",
    "                    # Update percentiles\n",
    "                    percentile_5 = np.percentile(post_change_points, 5)\n",
    "                    percentile_95 = np.percentile(post_change_points, 95)\n",
    "\n",
    "                post_change_points = []\n",
    "                # Add the current partition to data partitions\n",
    "                data_partitions.append(np.array(current_partition))\n",
    "                current_partition = []\n",
    "                change_detected = False\n",
    "                \n",
    "        \n",
    "    # Add any remaining points in current_partition to data_partitions\n",
    "    if current_partition:\n",
    "        data_partitions.append(np.array(current_partition))\n",
    "\n",
    "    \n",
    "    preprocessed_dict[name] = {\n",
    "        'name': name,\n",
    "        'data': data,\n",
    "        'label': label,\n",
    "        'data partitions': data_partitions,\n",
    "        'global_sliding_window': global_sw,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Anomaly Detection***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Isolation Forest***(Variant 2)\n",
    "Streaming variant with batch history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = 'IForest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for filename in preprocessed_dict.keys():\n",
    "    ts = preprocessed_dict[filename]\n",
    "    scores = []\n",
    "    previous_scores = None\n",
    "    x = ts['X_data']\n",
    "    clf = IForest(n_jobs=7, random_state=42)\n",
    "    total_time = 0\n",
    "    \n",
    "    for i, _ in enumerate(ts['batched_X_data']):\n",
    "        \n",
    "        if i == 0:\n",
    "            X_train = ts['batched_X_data'][i]\n",
    "        else:\n",
    "            X_train = np.concatenate((ts['batched_X_data'][i-1], ts['batched_X_data'][i]))\n",
    "        \n",
    "        t0 = time()\n",
    "        clf.fit(X_train)\n",
    "        score = clf.decision_scores_\n",
    "\n",
    "        if i > 0:\n",
    "            previous_partition_length = len(ts['batched_X_data'][i-1])\n",
    "            new_previous_scores = score[:previous_partition_length]\n",
    "            mean_previous_scores = (previous_scores + new_previous_scores) / 2\n",
    "            scores[-previous_partition_length:] = mean_previous_scores.tolist()\n",
    "\n",
    "        current_partition_length = len(ts['batched_X_data'][i])\n",
    "        current_scores = score[-current_partition_length:]\n",
    "        scores.extend(current_scores)\n",
    "\n",
    "        previous_scores = current_scores\n",
    "\n",
    "        t1 = time()\n",
    "\n",
    "        total_time += t1 - t0\n",
    "    \n",
    "    \n",
    "    scores = np.array(scores)\n",
    "    scores = MinMaxScaler(feature_range=(0, 1)).fit_transform(scores.reshape(-1, 1)).ravel()\n",
    "    scores = np.array([scores[0]] * math.ceil((ts['slidingWindow']-1)/2) +\n",
    "                         list(scores) +\n",
    "                         [scores[-1]] * ((ts['slidingWindow']-1)//2))\n",
    "    \n",
    "\n",
    "    # Plot figure\n",
    "    #plotFig(ts['data'], ts['label'], scores, ts['global_sliding_window'], fileName=ts['name'] + ' ' + loaded_dict[ts['name']][0], modelName=modelName)\n",
    "\n",
    "    # Calculate the results\n",
    "    L = printResult(ts['data'], ts['label'], scores, ts['slidingWindow'], ts['name'], modelName)\n",
    "    #L = [ '%.2f' % elem for elem in L]\n",
    "    #results.append([filename] + L)\n",
    "    results.append([filename] + L + [total_time, len(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns = ['Filename'] + eval_metrics\n",
    "columns = ['Name'] + ['AUC', 'Precision', 'Recall', 'F-score', 'Range-recall', 'ExistenceReward', 'OverlapReward', 'Range-precision', 'Range-Fscore', 'Precision@k', 'RangeAUC', 'Time', 'Number of Windows']\n",
    "iforest_res = pd.DataFrame(results, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Precision@k</th>\n",
       "      <th>Time</th>\n",
       "      <th>Number of Windows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ECG1</td>\n",
       "      <td>0.832550</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>243.160621</td>\n",
       "      <td>229801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ECG1_20k</td>\n",
       "      <td>0.831822</td>\n",
       "      <td>0.371852</td>\n",
       "      <td>21.308003</td>\n",
       "      <td>19901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IOPS1</td>\n",
       "      <td>0.481980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.723807</td>\n",
       "      <td>8497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SMD1</td>\n",
       "      <td>0.320700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.594669</td>\n",
       "      <td>28355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Occupancy1</td>\n",
       "      <td>0.763412</td>\n",
       "      <td>0.065844</td>\n",
       "      <td>2.820671</td>\n",
       "      <td>2541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ECG1+IOPS1</td>\n",
       "      <td>0.668095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.753022</td>\n",
       "      <td>28685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SMD1+Occupancy1</td>\n",
       "      <td>0.478901</td>\n",
       "      <td>0.039825</td>\n",
       "      <td>34.556695</td>\n",
       "      <td>31020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ECG1+IOPS1+Occupancy1</td>\n",
       "      <td>0.761818</td>\n",
       "      <td>0.031301</td>\n",
       "      <td>33.960795</td>\n",
       "      <td>31350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SMD1+ECG1+Occupancy1</td>\n",
       "      <td>0.623556</td>\n",
       "      <td>0.041465</td>\n",
       "      <td>56.347982</td>\n",
       "      <td>51020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ECG1+IOPS1+SMD1+Occupancy1</td>\n",
       "      <td>0.574619</td>\n",
       "      <td>0.017594</td>\n",
       "      <td>64.708527</td>\n",
       "      <td>59829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Name       AUC  Precision@k        Time  \\\n",
       "0                        ECG1  0.832550     0.155556  243.160621   \n",
       "1                    ECG1_20k  0.831822     0.371852   21.308003   \n",
       "2                       IOPS1  0.481980     0.000000   10.723807   \n",
       "3                        SMD1  0.320700     0.000000   31.594669   \n",
       "4                  Occupancy1  0.763412     0.065844    2.820671   \n",
       "5                  ECG1+IOPS1  0.668095     0.000000   31.753022   \n",
       "6             SMD1+Occupancy1  0.478901     0.039825   34.556695   \n",
       "7       ECG1+IOPS1+Occupancy1  0.761818     0.031301   33.960795   \n",
       "8        SMD1+ECG1+Occupancy1  0.623556     0.041465   56.347982   \n",
       "9  ECG1+IOPS1+SMD1+Occupancy1  0.574619     0.017594   64.708527   \n",
       "\n",
       "   Number of Windows  \n",
       "0             229801  \n",
       "1              19901  \n",
       "2               8497  \n",
       "3              28355  \n",
       "4               2541  \n",
       "5              28685  \n",
       "6              31020  \n",
       "7              31350  \n",
       "8              51020  \n",
       "9              59829  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iforest_res['Number of anomalies'] = iforest_res['Name'].apply(lambda x: np.sum(preprocessed_dict[x]['label']))\n",
    "iforest_res[['Name', 'AUC', 'Precision@k', 'Time', 'Number of Windows']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "                      Name &      AUC &  Precision@k &       Time &  Number of Windows \\\\\n",
      "\\midrule\n",
      "                      ECG1 & 0.832550 &     0.155556 & 243.160621 &             229801 \\\\\n",
      "                  ECG1\\_20k & 0.831822 &     0.371852 &  21.308003 &              19901 \\\\\n",
      "                     IOPS1 & 0.481980 &     0.000000 &  10.723807 &               8497 \\\\\n",
      "                      SMD1 & 0.320700 &     0.000000 &  31.594669 &              28355 \\\\\n",
      "                Occupancy1 & 0.763412 &     0.065844 &   2.820671 &               2541 \\\\\n",
      "                ECG1+IOPS1 & 0.668095 &     0.000000 &  31.753022 &              28685 \\\\\n",
      "           SMD1+Occupancy1 & 0.478901 &     0.039825 &  34.556695 &              31020 \\\\\n",
      "     ECG1+IOPS1+Occupancy1 & 0.761818 &     0.031301 &  33.960795 &              31350 \\\\\n",
      "      SMD1+ECG1+Occupancy1 & 0.623556 &     0.041465 &  56.347982 &              51020 \\\\\n",
      "ECG1+IOPS1+SMD1+Occupancy1 & 0.574619 &     0.017594 &  64.708527 &              59829 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(iforest_res[['Name', 'AUC', 'Precision@k', 'Time', 'Number of Windows']].to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data for the evaluation.\n",
    "all_data = []\n",
    "\n",
    "with open('dataset.pkl', 'rb') as f:\n",
    "    data = pkl.load(f)\n",
    "\n",
    "all_data.extend(data['evaluation']['single_normality'])\n",
    "all_data.extend(data['evaluation']['double_normality'])\n",
    "all_data.extend(data['evaluation']['triple_normality'])\n",
    "all_data.extend(data['evaluation']['quadruple_normality'])\n",
    "name_to_eval_series = {ts['Name']:ts for ts in all_data}\n",
    "\n",
    "tuning_data = []\n",
    "tuning_data.extend(data['tuning']['single_normality'])\n",
    "tuning_data.extend(data['tuning']['double_normality'])\n",
    "tuning_data.extend(data['tuning']['triple_normality'])\n",
    "tuning_data.extend(data['tuning']['quadruple_normality'])\n",
    "name_to_tune_series = {ts['Name']:ts for ts in tuning_data}\n",
    "\n",
    "# Set the number of windows to be fit per batch.\n",
    "windows_per_batch = 150\n",
    "\n",
    "def preprocess_series(series, slidingWindow=None, verbose=True):\n",
    "    # === Pre-processing steps ===\n",
    "\n",
    "    # Prepare data for unsupervised method\n",
    "    name = timeseries['Name']\n",
    "\n",
    "    data = timeseries['data']\n",
    "    max_length = data.shape[0]\n",
    "    label = timeseries['labels']\n",
    "\n",
    "    if slidingWindow is None:\n",
    "        slidingWindow = find_length(data)\n",
    "    X_data = Window(window=slidingWindow).convert(data).to_numpy()\n",
    "\n",
    "    # Take the series and batch it.\n",
    "    batched_data = []\n",
    "\n",
    "    i = 0\n",
    "    flag = True\n",
    "    # Keep taking batches until the point at which no new windows can be taken.\n",
    "    while i < len(data) and flag:\n",
    "        # The data batches begin at the index indicated. If first batch, then the beginning of the time series.\n",
    "        batch_samples_begin = i\n",
    "\n",
    "        # The data batches end at the index where `windows_per_batch` can be *completely* extracted since the batch beginning. \n",
    "        # Formula: \n",
    "        #   i: current beginning of batch / offset\n",
    "        #   + slidingWindow: to have enough samples extract one window\n",
    "        #   + windows_per_batch: to have enough samples to extract the rest of the windows\n",
    "        #   - 1: because the first window extracted is counted twice\n",
    "        batch_samples_end = i + windows_per_batch + slidingWindow - 1\n",
    "        \n",
    "        # Guard against the ending of the time series where a full batch cannot be formed.\n",
    "        if batch_samples_end > len(data):\n",
    "            batch_samples_end = len(data)\n",
    "            flag = False\n",
    " \n",
    "        # Guard against case where the batch cannot hold even one window.\n",
    "        if len(data[batch_samples_begin:batch_samples_end]) < slidingWindow:\n",
    "            break\n",
    "\n",
    "        batched_data.append(data[batch_samples_begin:batch_samples_end])\n",
    "\n",
    "        # The next batch starts at the point where a new window be created after the last window of the last batch.\n",
    "        # So, end of the previous window - length of window = start of the last window.\n",
    "        #   start of the last window + 1 = start of the first window of the next batch.\n",
    "        i = batch_samples_end - slidingWindow + 1\n",
    "\n",
    "\n",
    "    # Take the series and batch it for history batching: For each batch, also append have access to the data of the previous batch.\n",
    "    batched_data_previous_access = []\n",
    "    i = 0\n",
    "    previous_window_beginning = 0\n",
    "    flag = True\n",
    "    # Keep taking batches until the point at which no new windows can be taken.\n",
    "    while i < len(data) and flag:\n",
    "        # The data batches begin at the index indicated. If first batch, then the beginning of the time series.\n",
    "        batch_samples_begin = i\n",
    "\n",
    "        # The data batches end at the index where `windows_per_batch` can be *completely* extracted since the batch beginning. \n",
    "        # Formula: \n",
    "        #   i: current beginning of batch / offset\n",
    "        #   + slidingWindow: to have enough samples extract one window\n",
    "        #   + windows_per_batch: to have enough samples to extract the rest of the windows\n",
    "        #   - 1: because the first window extracted is counted twice\n",
    "        batch_samples_end = i + windows_per_batch + slidingWindow - 1\n",
    "        \n",
    "        # Guard against the ending of the time series where a full batch cannot be formed.\n",
    "        if batch_samples_end > len(data):\n",
    "            batch_samples_end = len(data)\n",
    "            flag = False\n",
    " \n",
    "        # Guard against case where the batch cannot hold even one window.\n",
    "        if len(data[batch_samples_begin:batch_samples_end]) < slidingWindow:\n",
    "            break\n",
    "\n",
    "        batched_data_previous_access.append(data[previous_window_beginning:batch_samples_end])\n",
    "\n",
    "        previous_window_beginning = batch_samples_begin\n",
    "\n",
    "        # The next batch starts at the point where a new window be created after the last window of the last batch.\n",
    "        # So, end of the previous window - length of window = start of the last window.\n",
    "        #   start of the last window + 1 = start of the first window of the next batch.\n",
    "        i = batch_samples_end - slidingWindow + 1\n",
    "\n",
    "    # Take the windows and batch them.\n",
    "    batched_X_data = []\n",
    "    i = 0\n",
    "    while i < len(X_data):\n",
    "        begin = i\n",
    "        end = i + windows_per_batch\n",
    "        if end > len(X_data):\n",
    "            end = len(X_data)\n",
    "\n",
    "        batched_X_data.append(X_data[begin:end])\n",
    "        i += windows_per_batch\n",
    "\n",
    "\n",
    "    # Processing data for dynamic partitioning.\n",
    "    initial_partition_length = int(len(data) * 0.25)\n",
    "    initial_partition = data[:initial_partition_length]\n",
    "\n",
    "    max_v = np.max(initial_partition)\n",
    "    min_v = np.min(initial_partition)\n",
    "\n",
    "    data_partitions = [initial_partition]\n",
    "    current_partition = []\n",
    "    change_detected = False\n",
    "\n",
    "    p = 500\n",
    "    change_point_threshold = 0.5\n",
    "    exceed_threshold = 0.65\n",
    "    post_change_points = []\n",
    "\n",
    "    for point in data[initial_partition_length:]:\n",
    "        \n",
    "        # Check for significant change\n",
    "        if (point > max_v * (1 + change_point_threshold)) or (point < min_v * (1 - change_point_threshold)):\n",
    "            change_detected = True\n",
    "     \n",
    "        current_partition.append(point)\n",
    "\n",
    "\n",
    "        # After change, collect additional points\n",
    "        if change_detected:\n",
    "            post_change_points.append(point)\n",
    "            if len(post_change_points) == p:\n",
    "                exceeds_threshold_points = [(pt > max_v * (1 + change_point_threshold) or pt < min_v * (1 - change_point_threshold)) for pt in post_change_points]\n",
    "                if sum(exceeds_threshold_points) >= exceed_threshold * p:\n",
    "                    max_v = np.mean([max_v] + [pt for pt in post_change_points if pt > max_v])\n",
    "                    min_v = np.mean([min_v] + [pt for pt in post_change_points if pt < min_v])\n",
    "\n",
    "                post_change_points = []\n",
    "\n",
    "                # Add the current partition to data partitions\n",
    "                data_partitions.append(np.array(current_partition))\n",
    "                current_partition = []\n",
    "                change_detected = False\n",
    "                \n",
    "        \n",
    "    # Add any remaining points in current_partition to data_partitions\n",
    "    if current_partition:\n",
    "        data_partitions.append(np.array(current_partition))\n",
    "\n",
    "    # Processing data for dynamic partitioning (Percentile variant)\n",
    "    initial_partition_length = int(len(data) * 0.25)\n",
    "    initial_partition = data[:initial_partition_length]\n",
    "\n",
    "    # Compute initial percentiles\n",
    "    percentile_5 = np.percentile(initial_partition, 5)\n",
    "    percentile_95 = np.percentile(initial_partition, 95)\n",
    "\n",
    "    percentile_data_partitions = []\n",
    "    current_partition = []\n",
    "    change_detected = False\n",
    "    p = 500\n",
    "    exceed_threshold = 0.5\n",
    "    post_change_points = []\n",
    "\n",
    "    for point in data[initial_partition_length:]:\n",
    "        \n",
    "        # Check for significant change\n",
    "        if (point < percentile_5) or (point > percentile_95):\n",
    "            change_detected = True\n",
    "     \n",
    "        current_partition.append(point)\n",
    "\n",
    "\n",
    "        # After change, collect additional points\n",
    "        if change_detected:\n",
    "            post_change_points.append(point)\n",
    "            if len(post_change_points) == p:\n",
    "                exceeds_threshold_points = [(pt < percentile_5 or pt > percentile_95) for pt in post_change_points]\n",
    "                if sum(exceeds_threshold_points) / p >= exceed_threshold:\n",
    "                    # Update percentiles\n",
    "                    percentile_5 = np.percentile(post_change_points, 5)\n",
    "                    percentile_95 = np.percentile(post_change_points, 95)\n",
    "\n",
    "                post_change_points = []\n",
    "                # Add the current partition to data partitions\n",
    "                percentile_data_partitions.append(np.array(current_partition))\n",
    "                current_partition = []\n",
    "                change_detected = False\n",
    "                \n",
    "        \n",
    "    # Add any remaining points in current_partition to percentile_data_partitions\n",
    "    if current_partition:\n",
    "        percentile_data_partitions.append(np.array(current_partition))\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Time-Series name: {name}')\n",
    "        print(\"Estimated Subsequence length: \", slidingWindow)\n",
    "        print()\n",
    "\n",
    "    return {\n",
    "        'name': name,\n",
    "        'data': data,\n",
    "        'label': label,\n",
    "        'slidingWindow': slidingWindow,\n",
    "        'X_data': X_data,\n",
    "        'batched_X_data': batched_X_data,\n",
    "        'batched_data': batched_data,\n",
    "        'points_per_batch': len(batched_data[0]),\n",
    "        'history_batched_data': batched_data_previous_access,\n",
    "        'dynamic_partitioning_batches': data_partitions,\n",
    "        'percentile_dynamic_partitioning_batches': percentile_data_partitions,\n",
    "        'Time series length': len(data),\n",
    "        'Number of abnormal points': list(label).count(1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "results = []\n",
    "\n",
    "# Parameters for tuning.\n",
    "param_grid = {\n",
    "    # Using the estimated window length from the autocorrelation, define alternate window sized as fractions/multiples of that.\n",
    "    'window_length_modifier': [0.1, 0.5, 1.0, 1.5, 2.0, 5.0], \n",
    "    # Number of trees in the isolation forest.\n",
    "    'n_estimators': [10, 20, 50, 100, 200, 500],\n",
    "}\n",
    "\n",
    "params_to_AUC = defaultdict(dict)\n",
    "\n",
    "total = np.product([len(pl) for pl in param_grid.values()])\n",
    "\n",
    "for timeseries in (p := tqdm(tuning_data)):\n",
    "    name = timeseries['Name']\n",
    "\n",
    "    p.set_description(name)\n",
    "\n",
    "    c = 0\n",
    "    best_AUC = 0\n",
    "    # Initial Best parameters are the defaults.\n",
    "    best_params = (1, 100)\n",
    "    for window_length_modifier in param_grid['window_length_modifier']:\n",
    "        for n_estimators in param_grid['n_estimators']:\n",
    "            # Prevent too small windows.\n",
    "            window_size = max(10, int(window_length_modifier * ts['slidingWindow']))\n",
    "\n",
    "            ts = preprocess_series(series=timeseries, slidingWindow=window_size, verbose=False)\n",
    "\n",
    "            x = ts['X_data']\n",
    "            clf = IForest(n_jobs=10, random_state=42, n_estimators=n_estimators)\n",
    "            total_time = 0\n",
    "            scores = []\n",
    "\n",
    "            for i, _ in enumerate(ts['batched_X_data']):\n",
    "                \n",
    "                if i == 0:\n",
    "                    X_train = ts['batched_X_data'][i]\n",
    "                else:\n",
    "                    X_train = np.concatenate((ts['batched_X_data'][i-1], ts['batched_X_data'][i]))\n",
    "                \n",
    "                t0 = time()\n",
    "                clf.fit(X_train)\n",
    "                score = clf.decision_scores_\n",
    "\n",
    "                if i > 0:\n",
    "                    previous_partition_length = len(ts['batched_X_data'][i-1])\n",
    "                    new_previous_scores = score[:previous_partition_length]\n",
    "                    mean_previous_scores = (previous_scores + new_previous_scores) / 2\n",
    "                    scores[-previous_partition_length:] = mean_previous_scores.tolist()\n",
    "\n",
    "                current_partition_length = len(ts['batched_X_data'][i])\n",
    "                current_scores = score[-current_partition_length:]\n",
    "                scores.extend(current_scores)\n",
    "\n",
    "                previous_scores = current_scores\n",
    "\n",
    "                t1 = time()\n",
    "\n",
    "                total_time += t1 - t0\n",
    "\n",
    "            score = np.array(score)\n",
    "            score = MinMaxScaler(feature_range=(0,1)).fit_transform(score.reshape(-1,1)).ravel()\n",
    "            score = np.array([score[0]]*math.ceil((ts['slidingWindow']-1)/2) + list(score) + [score[-1]]*((ts['slidingWindow']-1)//2))\n",
    "            \n",
    "            AUC = printResult(ts['data'], ts['label'], score, window_size, ts['name'], modelName)[0]\n",
    "\n",
    "            params_to_AUC[name][(window_length_modifier, n_estimators)] = AUC\n",
    "\n",
    "            if AUC > best_AUC:\n",
    "                best_AUC = AUC\n",
    "                best_params = (window_length_modifier, n_estimators)\n",
    "\n",
    "            c+=1\n",
    "            print(f\"\\r[{c}/{total}]{name}  --  Best AUC = {best_AUC} for: {best_params}\", end='')\n",
    "    print()\n",
    "    print(f\"{name}  --  Best AUC = {best_AUC} for: {best_params}\")\n",
    "\n",
    "    # Evaluate evaluation time series with selected parameters.    \n",
    "    window_size = max(10, int(ts['slidingWindow'] * best_params[0]))\n",
    "    n_estimators = best_params[1]\n",
    "\n",
    "    eval_series_name = ''.join([n if n!='2' else '1' for n in name]).replace('10k', '20k')  # Replace 2s with 1s and fix 20k becoming 10k accidentally.\n",
    "    ts = preprocess_series(series=timeseries, slidingWindow=window_size, verbose=False)\n",
    "\n",
    "    x = ts['X_data']\n",
    "    clf = IForest(n_jobs=10, random_state=42, n_estimators=n_estimators)\n",
    "    total_time = 0\n",
    "    scores = []\n",
    "\n",
    "    for i, _ in enumerate(ts['batched_X_data']):\n",
    "        \n",
    "        if i == 0:\n",
    "            X_train = ts['batched_X_data'][i]\n",
    "        else:\n",
    "            X_train = np.concatenate((ts['batched_X_data'][i-1], ts['batched_X_data'][i]))\n",
    "        \n",
    "        t0 = time()\n",
    "        clf.fit(X_train)\n",
    "        score = clf.decision_scores_\n",
    "\n",
    "        if i > 0:\n",
    "            previous_partition_length = len(ts['batched_X_data'][i-1])\n",
    "            new_previous_scores = score[:previous_partition_length]\n",
    "            mean_previous_scores = (previous_scores + new_previous_scores) / 2\n",
    "            scores[-previous_partition_length:] = mean_previous_scores.tolist()\n",
    "\n",
    "        current_partition_length = len(ts['batched_X_data'][i])\n",
    "        current_scores = score[-current_partition_length:]\n",
    "        scores.extend(current_scores)\n",
    "\n",
    "        previous_scores = current_scores\n",
    "\n",
    "        t1 = time()\n",
    "\n",
    "        total_time += t1 - t0\n",
    "\n",
    "    score = np.array(score)\n",
    "    score = MinMaxScaler(feature_range=(0,1)).fit_transform(score.reshape(-1,1)).ravel()\n",
    "    score = np.array([score[0]]*math.ceil((ts['slidingWindow']-1)/2) + list(score) + [score[-1]]*((ts['slidingWindow']-1)//2))\n",
    "    \n",
    "    L = printResult(ts['data'], ts['label'], score, window_size, ts['name'], modelName)\n",
    "    print(f\"{eval_series_name}  --  Eval AUC = {L[0]}\")\n",
    "    results.append([name] + L + [t1-t0, len(x)])\n",
    "\n",
    "    print()\n",
    "    print('----------------------------------------------------------------')\n",
    "    sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
