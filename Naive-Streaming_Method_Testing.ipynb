{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TSB_UAD.models.distance import Fourier\n",
    "from TSB_UAD.models.feature import Window\n",
    "from TSB_UAD.utils.slidingWindows import find_length, plotFig, printResult\n",
    "\n",
    "from TSB_UAD.models.iforest import IForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Naive-Streaming Methods***\n",
    "TODO: Add comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Data Pre-Processing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Time-Series dictionary\n",
    "with open('Time-Series-Data-Dictionary.json', 'r') as json_file:\n",
    "    loaded_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename, info in loaded_dict.items():\n",
    "    print(f'{filename}: {info}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from more_itertools import chunked\n",
    "\n",
    "# Set the number of windows to be fit per batch.\n",
    "windows_per_batch = 200\n",
    "\n",
    "for filename, info in loaded_dict.items():\n",
    "    ts_filepath = f\"TS-Data-Files/{filename}\"\n",
    "    \n",
    "    # === Pre-processing steps ===\n",
    "\n",
    "    # Prepare data for unsupervised method\n",
    "    ts = pd.read_csv(ts_filepath, header=None).dropna().to_numpy()\n",
    "\n",
    "    name = ts_filepath.split('/')[-1]\n",
    "    max_length = ts.shape[0]\n",
    "\n",
    "    data = ts[:max_length, 0].astype(float)\n",
    "    label = ts[:max_length, 1]\n",
    "\n",
    "    slidingWindow = find_length(data)\n",
    "    X_data = Window(window=slidingWindow).convert(data).to_numpy()\n",
    "\n",
    "    # Take the series and batch it.\n",
    "    batched_data = []\n",
    "    i = 0\n",
    "    while i < len(data):\n",
    "        # print(len(data), i, i+slidingWindow+windows_per_batch, len(data[i:i+slidingWindow+windows_per_batch]))\n",
    "        batched_data.append(data[i:i+slidingWindow+windows_per_batch - 1])\n",
    "        i += windows_per_batch\n",
    "        if i+slidingWindow+windows_per_batch > len(data):\n",
    "            batched_data.append(data[i:min(i+slidingWindow+windows_per_batch - 1, len(data))])\n",
    "            # print('end', len(data), i, i+slidingWindow+windows_per_batch, len(data[i:i+slidingWindow+windows_per_batch]))\n",
    "            break\n",
    "        # print(len(data), i, i+slidingWindow+windows_per_batch, len(data[i:i+slidingWindow+windows_per_batch]))\n",
    "\n",
    "        \n",
    "    # Take the windows and batch them.\n",
    "    batched_X_data = [np.array(list(batch)) for batch in chunked(X_data, windows_per_batch)]\n",
    "\n",
    "    # Prepare data for semisupervised method. \n",
    "    # Here, the training ratio = 0.1\n",
    "\n",
    "    data_train = data[:int(0.1 * len(data))]\n",
    "    data_test = data\n",
    "\n",
    "    X_train = Window(window=slidingWindow).convert(data_train).to_numpy()\n",
    "    X_test = Window(window=slidingWindow).convert(data_test).to_numpy()\n",
    "\n",
    "    print(f'Time-Series filename: {filename}')\n",
    "    print(\"Estimated Subsequence length: \", slidingWindow)\n",
    "    print()\n",
    "    \n",
    "    # Store the pre-processed variables in the new dictionary\n",
    "    preprocessed_dict[filename] = {\n",
    "        'name': name,\n",
    "        'data': data,\n",
    "        'batched_data': batched_data,\n",
    "        'label': label,\n",
    "        'slidingWindow': slidingWindow,\n",
    "        'X_data': X_data,\n",
    "        'batched_X_data': batched_X_data,\n",
    "        'data_train': data_train,\n",
    "        'data_test': data_test,\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'Time series length': len(data),\n",
    "        'Number of abnormal points': list(label).count(1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "265-125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preprocessed_dict['ts1']['batched_X_data'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "len(preprocessed_dict['ts1']['batched_data'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "65+65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get filenames, time series lengths, and number of abnormal points\n",
    "filenames = list(preprocessed_dict.keys())\n",
    "time_series_lengths = [data['Time series length'] for data in preprocessed_dict.values()]\n",
    "number_of_abnormal_points = [data['Number of abnormal points'] for data in preprocessed_dict.values()]\n",
    "\n",
    "# Plot 'Time series length' and 'Number of abnormal points' for each filename\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(filenames, time_series_lengths, marker='o', linestyle='-', color='skyblue')\n",
    "plt.xlabel('Filename')\n",
    "plt.ylabel('Time series length')\n",
    "plt.title('Time Series Length for Each Filename')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(filenames, number_of_abnormal_points, marker='o', linestyle='-', color='lightgreen')\n",
    "plt.xlabel('Filename')\n",
    "plt.ylabel('Number of abnormal points')\n",
    "plt.title('Number of Abnormal Points for Each Filename')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Anomaly Detection***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Isolation Forest***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "modelName = 'IForest'\n",
    "clf = IForest(n_jobs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for filename in tqdm(preprocessed_dict.keys(), desc='Processing Time Series'):\n",
    "    ts = preprocessed_dict[filename]\n",
    "    x = ts['X_data']\n",
    "    \n",
    "    score = []\n",
    "    for batch in tqdm(ts['batched_X_data'], desc='Processing Batch'):\n",
    "        clf.fit(batch)\n",
    "        score.extend(clf.decision_scores_)\n",
    "    \n",
    "    score = np.array(score)\n",
    "    score = MinMaxScaler(feature_range=(0,1)).fit_transform(score.reshape(-1,1)).ravel()\n",
    "    score = np.array([score[0]]*math.ceil((ts['slidingWindow']-1)/2) + list(score) + [score[-1]]*((ts['slidingWindow']-1)//2))\n",
    "    \n",
    "    L = printResult(ts['data'], ts['label'], score, ts['slidingWindow'], ts['name'], modelName)\n",
    "    results.append([filename] + L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = ['Filename'] + [f'Metric_{i+1}' for i in range(len(results[0])-1)]\n",
    "columns = ['Filename'] + ['AUC', 'Precision', 'Recall', 'F-score', 'Range-recall', 'ExistenceReward', 'OverlapReward', 'Range-precision', 'Range-Fscore', 'Precison@k', 'RangeAUC']\n",
    "df = pd.DataFrame(results, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***STUMP***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stumpy\n",
    "modelName = 'STUMP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for filename in tqdm(preprocessed_dict.keys(), desc='Processing Time Series'):\n",
    "    ts = preprocessed_dict[filename]\n",
    "    full_ts = ts['data']\n",
    "    subseries = ts['X_data']\n",
    "    window_size = ts['slidingWindow']\n",
    "    slidingWindow = window_size\n",
    "\n",
    "    k = 1\n",
    "    score = []\n",
    "    for batch in tqdm(ts['batched_data'], desc='Processing Batch'):\n",
    "        score_ = stumpy.stump(T_A=batch, m=window_size, k=k, ignore_trivial=True, normalize=True)\n",
    "    \n",
    "        score.extend(score_.T[k-1])\n",
    "        \n",
    "    score = np.array(score)\n",
    "    score = MinMaxScaler(feature_range=(0,1)).fit_transform(score.reshape(-1,1)).ravel()\n",
    "    score = np.array([score[0]]*math.ceil((slidingWindow-1)/2) + list(score) + [score[-1]]*((slidingWindow-1)//2))\n",
    "    \n",
    "    L = printResult(ts['data'], ts['label'], score, ts['slidingWindow'], ts['name'], modelName)\n",
    "    results.append([filename] + L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Filename'] + ['AUC', 'Precision', 'Recall', 'F-score', 'Range-recall', 'ExistenceReward', 'OverlapReward', 'Range-precision', 'Range-Fscore', 'Precison@k', 'RangeAUC']\n",
    "df = pd.DataFrame(results, columns=columns)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
